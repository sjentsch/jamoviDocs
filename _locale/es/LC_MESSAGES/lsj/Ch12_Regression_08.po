# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The section authors, The jamovi Group, and Sebastian
# Jentschke (curating this documentation). This work is licensed under a
# Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: jamovi \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-08-27 11:03+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../lsj/Ch12_Regression_08.rst:4
msgid "Regarding regression coefficients"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:6
msgid ""
"Before moving on to discuss the assumptions underlying linear regression "
"and what you can do to check if they’re being met, there’s two more "
"topics I want to briefly discuss, both of which relate to the regression "
"coefficients. The first thing to talk about is calculating confidence "
"intervals for the coefficients. After that, I’ll discuss the somewhat "
"murky question of how to determine which predictor is most important."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:14
msgid "Confidence intervals for the coefficients"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:16
msgid ""
"Like any population parameter, the regression coefficients *b* cannot be "
"estimated with complete precision from a sample of data; that’s part of "
"why we need hypothesis tests. Given this, it’s quite useful to be able to"
" report confidence intervals that capture our uncertainty about the true "
"value of *b*. This is especially useful when the research question "
"focuses heavily on an attempt to find out *how* strongly variable *X* is "
"related to variable *Y*, since in those situations the interest is "
"primarily in the regression weight *b*."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:26
msgid ""
"Fortunately, confidence intervals for the regression weights can be "
"constructed in the usual fashion"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:29
msgid ""
"\\mbox{CI}(b) = \\hat{b} \\pm \\left( t_{crit} \\times "
"\\mbox{\\textsc{se}}(\\hat{b})  \\right)\n"
"\n"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:31
#, python-format
msgid ""
"where :math:`\\mbox{\\textsc{se}}(\\hat{b})` is the standard error of the"
" regression coefficient, and *t*\\ :sub:`crit` is the relevant critical "
"value of the appropriate *t* distribution. For instance, if it’s a 95% "
"confidence interval that we want, then the critical value is the 97.5th "
"quantile of a *t* distribution with *N* - K - 1 degrees of freedom. In "
"other words, this is basically the same approach to calculating "
"confidence intervals that we’ve used throughout."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:39
msgid ""
"In jamovi we had already specified the ``95% Confidence interval`` as "
"shown if :numref:`fig-reg2`, although we could easily have chosen another"
" value, say a ``99% Confidence interval`` if that is what we decided on."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:44
msgid "Calculating standardised regression coefficients"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:46
msgid ""
"One more thing that you might want to do is to calculate “standardised” "
"regression coefficients, often denoted *β*. The rationale behind "
"standardised coefficients goes like this. In a lot of situations, your "
"variables are on fundamentally different scales. Suppose, for example, my"
" regression model aims to predict people’s IQ scores using their "
"educational attainment (number of years of education) and their income as"
" predictors. Obviously, educational attainment and income are not on the "
"same scales. The number of years of schooling might only vary by 10s of "
"years, whereas income can vary by 10,000s of dollars (or more). The units"
" of measurement have a big influence on the regression coefficients. The "
"*b* coefficients only make sense when interpreted in light of the units, "
"both of the predictor variables and the outcome variable. This makes it "
"very difficult to compare the coefficients of different predictors. Yet "
"there are situations where you really do want to make comparisons between"
" different coefficients. Specifically, you might want some kind of "
"standard measure of which predictors have the strongest relationship to "
"the outcome. This is what **standardised coefficients** aim to do."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:65
msgid ""
"The basic idea is quite simple; the standardised coefficients are the "
"coefficients that you would have obtained if you’d converted all the "
"variables to *z*-scores before running the regression.\\ [#]_ The idea "
"here is that, by converting all the predictors to *z*-scores, they all go"
" into the regression on the same scale, thereby removing the problem of "
"having variables on different scales. Regardless of what the original "
"variables were, a *β* value of 1 means that an increase in the predictor "
"of 1 standard deviation will produce a corresponding 1 standard deviation"
" increase in the outcome variable. Therefore, if variable A has a larger "
"absolute value of *β* than variable B, it is deemed to have a stronger "
"relationship with the outcome. Or at least that’s the idea. It’s worth "
"being a little cautious here, since this does rely very heavily on the "
"assumption that “a 1 standard deviation change” is fundamentally the same"
" kind of thing for all variables. It’s not always obvious that this is "
"true."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:81
msgid ""
"Leaving aside the interpretation issues, let’s look at how it’s "
"calculated. What you could do is standardise all the variables yourself "
"and then run a regression, but there’s a much simpler way to do it. As it"
" turns out, the *β* coefficient for a predictor *X* and outcome *Y* has a"
" very simple formula, namely"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:87
msgid "β\\ :sub:`X` = *b*\\ :sub:`X` × (σ\\ :sub:`X` / σ\\ :sub:`Y`)"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:89
msgid ""
"where σ\\ :sub:`X` is the standard deviation of the predictor, and σ\\ "
":sub:`Y` is the standard deviation of the outcome variable *Y*. This "
"makes matters a lot simpler."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:93
msgid ""
"To make things even simpler, jamovi has an option that computes the *β* "
"coefficients for you using the ``Standardized estimate`` checkbox in the "
"``Model Coefficients`` options, see results in :numref:`fig-reg3`."
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:103
#, python-format
msgid ""
"Standardised coefficients, with 95% confidence intervals, for multiple "
"linear regression"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:108
msgid ""
"This clearly shows that the ``dan.sleep`` variable has a much stronger "
"effect than the ``baby.sleep`` variable. However, this is a perfect "
"example of a situation where it would probably make sense to use the "
"original coefficients *b* rather than the standardised coefficients *β*. "
"After all, my sleep and the baby’s sleep are *already* on the same scale:"
" number of hours slept. Why complicate matters by converting these to "
"*z*-scores?"
msgstr ""

#: ../../lsj/Ch12_Regression_08.rst:119
msgid ""
"Strictly, you standardise all the *regressors*. That is, every “thing” "
"that has a regression coefficient associated with it in the model. For "
"the regression models that I’ve talked about so far, each predictor "
"variable maps onto exactly one regressor, and vice versa. However, that’s"
" not actually true in general and we’ll see some examples of this in "
"Chapter `Factorial ANOVA <Ch14_ANOVA2.html#factorial-anova>`__. But, for "
"now we don’t need to care too much about this distinction."
msgstr ""

