# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The section authors, The jamovi Group, and Sebastian Jentschke (curating this documentation). This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: jamovi\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-08-21 00:07+0200\n"
"PO-Revision-Date: 2020-08-10 17:54+0000\n"
"Language-Team: Turkish (https://www.transifex.com/jamovi/teams/111618/tr/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: tr\n"
"Plural-Forms: nplurals=2; plural=(n > 1);\n"

#: ../../lsj/Ch02_StudyDesign_5.rst:4
msgid "Assessing the validity of a study"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:6
msgid ""
"More than any other thing, a scientist wants their research to be “valid”. "
"The conceptual idea behind **validity** is very simple. Can you trust the "
"results of your study? If not, the study is invalid. However, whilst it’s "
"easy to state, in practice it’s much harder to check validity than it is to "
"check reliability. And in all honesty, there’s no precise, clearly agreed "
"upon notion of what validity actually is. In fact, there are lots of "
"different kinds of validity, each of which raises it’s own issues. And not "
"all forms of validity are relevant to all studies. I’m going to talk about "
"five different types of validity:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:16 ../../lsj/Ch02_StudyDesign_5.rst:36
msgid "Internal validity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:18 ../../lsj/Ch02_StudyDesign_5.rst:62
msgid "External validity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:20 ../../lsj/Ch02_StudyDesign_5.rst:125
msgid "Construct validity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:22 ../../lsj/Ch02_StudyDesign_5.rst:147
msgid "Face validity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:24 ../../lsj/Ch02_StudyDesign_5.rst:192
msgid "Ecological validity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:26
msgid ""
"First, a quick guide as to what matters here. (1) Internal and external "
"validity are the most important, since they tie directly to the fundamental "
"question of whether your study really works. (2) Construct validity asks "
"whether you’re measuring what you think you are. (3) Face validity isn’t "
"terribly important except insofar as you care about “appearances”. (4) "
"Ecological validity is a special case of face validity that corresponds to a"
" kind of appearance that you might care about a lot."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:38
msgid ""
"**Internal validity** refers to the extent to which you are able draw the "
"correct conclusions about the causal relationships between variables. It’s "
"called “internal” because it refers to the relationships between things "
"“inside” the study. Let’s illustrate the concept with a simple example. "
"Suppose you’re interested in finding out whether a university education "
"makes you write better. To do so, you get a group of first year students, "
"ask them to write a 1000 word essay, and count the number of spelling and "
"grammatical errors they make. Then you find some third-year students, who "
"obviously have had more of a university education than the first-years, and "
"repeat the exercise. And let’s suppose it turns out that the third-year "
"students produce fewer errors. And so you conclude that a university "
"education improves writing skills. Right? Except that the big problem with "
"this experiment is that the third-year students are older and they’ve had "
"more experience with writing things. So it’s hard to know for sure what the "
"causal relationship is. Do older people write better? Or people who have had"
" more writing experience? Or people who have had more education? Which of "
"the above is the true *cause* of the superior performance of the third-"
"years? Age? Experience? Education? You can’t tell. This is an example of a "
"failure of internal validity, because your study doesn’t properly tease "
"apart the *causal* relationships between the different variables."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:64
msgid ""
"**External validity** relates to the **generalisability** or "
"**applicability** of your findings. That is, to what extent do you expect to"
" see the same pattern of results in “real life” as you saw in your study. To"
" put it a bit more precisely, any study that you do in psychology will "
"involve a fairly specific set of questions or tasks, will occur in a "
"specific environment, and will involve participants that are drawn from a "
"particular subgroup (disappointingly often it is college students!). So, if "
"it turns out that the results don’t actually generalise or apply to people "
"and situations beyond the ones that you studied, then what you’ve got is a "
"lack of external validity."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:75
msgid ""
"The classic example of this issue is the fact that a very large proportion "
"of studies in psychology will use undergraduate psychology students as the "
"participants. Obviously, however, the researchers don’t care *only* about "
"psychology students. They care about people in general. Given that, a study "
"that uses only psychology students as participants always carries a risk of "
"lacking external validity. That is, if there’s something “special” about "
"psychology students that makes them different to the general population in "
"some *relevant* respect, then we may start worrying about a lack of external"
" validity."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:85
msgid ""
"That said, it is absolutely critical to realise that a study that uses only "
"psychology students does not necessarily have a problem with external "
"validity. I’ll talk about this again later, but it’s such a common mistake "
"that I’m going to mention it here. The external validity of a study is "
"threatened by the choice of population if (a) the population from which you "
"sample your participants is very narrow (e.g., psychology students), and (b)"
" the narrow population that you sampled from is systematically different "
"from the general population *in some respect that is relevant to the "
"psychological phenomenon that you intend to study*. The italicised part is "
"the bit that lots of people forget. It is true that psychology "
"undergraduates differ from the general population in lots of ways, and so a "
"study that uses only psychology students *may* have problems with external "
"validity. However, if those differences aren’t very relevant to the "
"phenomenon that you’re studying, then there’s nothing to worry about. To "
"make this a bit more concrete here are two extreme examples:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:102
msgid ""
"You want to measure “attitudes of the general public towards psychotherapy”,"
" but all of your participants are psychology students. This study would "
"almost certainly have a problem with external validity."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:107
msgid ""
"You want to measure the effectiveness of a visual illusion, and your "
"participants are all psychology students. This study is unlikely to have a "
"problem with external validity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:111
msgid ""
"Having just spent the last couple of paragraphs focusing on the choice of "
"participants, since that’s a big issue that everyone tends to worry most "
"about, it’s worth remembering that external validity is a broader concept. "
"The following are also examples of things that might pose a threat to "
"external validity, depending on what kind of study you’re doing:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:118
msgid ""
"People might answer a “psychology questionnaire” in a manner that doesn’t "
"reflect what they would do in real life."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:121
msgid ""
"Your lab experiment on (say) “human learning” has a different structure to "
"the learning problems people face in real life."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:127
msgid ""
"**Construct validity** is basically a question of whether you’re measuring "
"what you want to be measuring. A measurement has good construct validity if "
"it is actually measuring the correct theoretical construct, and bad "
"construct validity if it doesn’t. To give a very simple (if ridiculous) "
"example, suppose I’m trying to investigate the rates with which university "
"students cheat on their exams. And the way I attempt to measure it is by "
"asking the cheating students to stand up in the lecture theatre so that I "
"can count them. When I do this with a class of 300 students 0 people claim "
"to be cheaters. So I therefore conclude that the proportion of cheaters in "
"my class is 0%. Clearly this is a bit ridiculous. But the point here is not "
"that this is a very deep methodological example, but rather to explain what "
"construct validity is. The problem with my measure is that while I’m "
"*trying* to measure “the proportion of people who cheat” what I’m actually "
"measuring is “the proportion of people stupid enough to own up to cheating, "
"or bloody minded enough to pretend that they do”. Obviously, these aren’t "
"the same thing! So my study has gone wrong, because my measurement has very "
"poor construct validity."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:149
msgid ""
"**Face validity** simply refers to whether or not a measure “looks like” "
"it’s doing what it’s supposed to, nothing more. If I design a test of "
"intelligence, and people look at it and they say “no, that test doesn’t "
"measure intelligence”, then the measure lacks face validity. It’s as simple "
"as that. Obviously, face validity isn’t very important from a pure "
"scientific perspective. After all, what we care about is whether or not the "
"measure *actually* does what it’s supposed to do, not whether it *looks "
"like* it does what it’s supposed to do. As a consequence, we generally don’t"
" care very much about face validity. That said, the concept of face validity"
" serves three useful pragmatic purposes:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:160
msgid ""
"Sometimes, an experienced scientist will have a “hunch” that a particular "
"measure won’t work. While these sorts of hunches have no strict evidentiary "
"value, it’s often worth paying attention to them. Because often times people"
" have knowledge that they can’t quite verbalise, so there might be something"
" to worry about even if you can’t quite say why. In other words, when "
"someone you trust criticises the face validity of your study, it’s worth "
"taking the time to think more carefully about your design to see if you can "
"think of reasons why it might go awry. Mind you, if you don’t find any "
"reason for concern, then you should probably not worry. After all, face "
"validity really doesn’t matter very much."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:172
msgid ""
"Often (very often), completely uninformed people will also have a “hunch” "
"that your research is crap. And they’ll criticise it on the internet or "
"something. On close inspection you may notice that these criticisms are "
"actually focused entirely on how the study “looks”, but not on anything "
"deeper. The concept of face validity is useful for gently explaining to "
"people that they need to substantiate their arguments further."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:180
msgid ""
"Expanding on the last point, if the beliefs of untrained people are critical"
" (e.g., this is often the case for applied research where you actually want "
"to convince policy makers of something or other) then you *have* to care "
"about face validity. Simply because, whether you like it or not, a lot of "
"people will use face validity as a proxy for real validity. If you want the "
"government to change a law on scientific psychological grounds, then it "
"won’t matter how good your studies “really” are. If they lack face validity "
"you’ll find that politicians ignore you. Of course, it’s somewhat unfair "
"that policy often depends more on appearance than fact, but that’s how "
"things go."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:194
msgid ""
"**Ecological validity** is a different notion of validity, which is similar "
"to external validity, but less important. The idea is that, in order to be "
"ecologically valid, the entire set up of the study should closely "
"approximate the real world scenario that is being investigated. In a sense, "
"ecological validity is a kind of face validity. It relates mostly to whether"
" the study “looks” right, but with a bit more rigour to it. To be "
"ecologically valid the study has to look right in a fairly specific way. The"
" idea behind it is the intuition that a study that is ecologically valid is "
"more likely to be externally valid. It’s no guarantee, of course. But the "
"nice thing about ecological validity is that it’s much easier to check "
"whether a study is ecologically valid than it is to check whether a study is"
" externally valid. A simple example would be eyewitness identification "
"studies. Most of these studies tend to be done in a university setting, "
"often with a fairly simple array of faces to look at, rather than a line up."
" The length of time between seeing the “criminal” and being asked to "
"identify the suspect in the “line up” is usually shorter. The “crime” isn’t "
"real so there’s no chance of the witness being scared, and there are no "
"police officers present so there’s not as much chance of feeling pressured. "
"These things all mean that the study *definitely* lacks ecological validity."
" They might (but might not) mean that it also lacks external validity."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:218
msgid "Confounds, artefacts and other threats to validity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:220
msgid ""
"If we look at the issue of validity in the most general fashion the two "
"biggest worries that we have are *confounders* and *artefacts*. These two "
"terms are defined in the following way:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:224
msgid ""
"**Confounder**: A confounder is an additional, often unmeasured variable\\ "
"[#]_ that turns out to be related to both the predictors and the outcome. "
"The existence of confounders threatens the internal validity of the study "
"because you can’t tell whether the predictor causes the outcome, or if the "
"confounding variable causes it."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:230
msgid ""
"**Artefact**: A result is said to be “artefactual” if it only holds in the "
"special situation that you happened to test in your study. The possibility "
"that your result is an artefact describes a threat to your external "
"validity, because it raises the possibility that you can’t generalise or "
"apply your results to the actual population that you care about."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:237
msgid ""
"As a general rule confounders are a bigger concern for non-experimental "
"studies, precisely because they’re not proper experiments. By definition, "
"you’re leaving lots of things uncontrolled, so there’s a lot of scope for "
"confounders being present in your study. Experimental research tends to be "
"much less vulnerable to confounders. The more control you have over what "
"happens during the study, the more you can prevent confounders from "
"affecting the results. With random allocation, for example, confounders are "
"distributed randomly, and evenly, between different groups."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:247
msgid ""
"However, there are always swings and roundabouts and when we start thinking "
"about artefacts rather than confounders the shoe is very firmly on the other"
" foot. For the most part, artefactual results tend to be a concern for "
"experimental studies than for non-experimental studies. To see this, it "
"helps to realise that the reason that a lot of studies are non-experimental "
"is precisely because what the researcher is trying to do is examine human "
"behaviour in a more naturalistic context. By working in a more real-world "
"context you lose experimental control (making yourself vulnerable to "
"confounders), but because you tend to be studying human psychology “in the "
"wild” you reduce the chances of getting an artefactual result. Or, to put it"
" another way, when you take psychology out of the wild and bring it into the"
" lab (which we usually have to do to gain our experimental control), you "
"always run the risk of accidentally studying something different to what you"
" wanted to study."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:262
msgid ""
"Be warned though. The above is a rough guide only. It’s absolutely possible "
"to have confounders in an experiment, and to get artefactual results with "
"non-experimental studies. This can happen for all sorts of reasons, not "
"least of which is experimenter or researcher error. In practice, it’s really"
" hard to think everything through ahead of time and even very good "
"researchers make mistakes."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:269
msgid ""
"Although there’s a sense in which almost any threat to validity can be "
"characterised as a confounder or an artefact, they’re pretty vague concepts."
" So let’s have a look at some of the most common examples."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:274
msgid "History effects"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:276
msgid ""
"**History effects** refer to the possibility that specific events may occur "
"during the study that might influence the outcome measure. For instance, "
"something might happen in between a pre-test and a post-test. Or in-between "
"testing participant 23 and participant 24. Alternatively, it might be that "
"you’re looking at a paper from an older study that was perfectly valid for "
"its time, but the world has changed enough since then that the conclusions "
"are no longer trustworthy. Examples of things that would count as history "
"effects are:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:285
msgid ""
"You’re interested in how people think about risk and uncertainty. You "
"started your data collection in December 2010. But finding participants and "
"collecting data takes time, so you’re still finding new people in February "
"2011. Unfortunately for you (and even more unfortunately for others), the "
"Queensland floods occurred in January 2011 causing billions of dollars of "
"damage and killing many people. Not surprisingly, the people tested in "
"February 2011 express quite different beliefs about handling risk than the "
"people tested in December 2010. Which (if any) of these reflects the “true” "
"beliefs of participants? I think the answer is probably both. The Queensland"
" floods genuinely changed the beliefs of the Australian public, though "
"possibly only temporarily. The key thing here is that the “history” of the "
"people tested in February is quite different to people tested in December."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:300
msgid ""
"You’re testing the psychological effects of a new anti-anxiety drug. So what"
" you do is measure anxiety before administering the drug (e.g., by self-"
"report, and taking physiological measures). Then you administer the drug, "
"and afterwards you take the same measures. In the middle however, because "
"your lab is in Los Angeles, there’s an earthquake which increases the "
"anxiety of the participants."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:308
msgid "Maturation effects"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:310
msgid ""
"As with history effects, **maturational effects** are fundamentally about "
"change over time. However, maturation effects aren’t in response to specific"
" events. Rather, they relate to how people change on their own over time. We"
" get older, we get tired, we get bored, etc. Some examples of maturation "
"effects are:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:316
msgid ""
"When doing developmental psychology research you need to be aware that "
"children grow up quite rapidly. So, suppose that you want to find out "
"whether some educational trick helps with vocabulary size among 3 year olds."
" One thing that you need to be aware of is that the vocabulary size of "
"children that age is growing at an incredible rate (multiple words per day) "
"all on its own. If you design your study without taking this maturational "
"effect into account, then you won’t be able to tell if your educational "
"trick works."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:325
msgid ""
"When running a very long experiment in the lab (say, something that goes for"
" 3 hours) it’s very likely that people will begin to get bored and tired, "
"and that this maturational effect will cause performance to decline "
"regardless of anything else going on in the experiment"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:332
msgid "Repeated testing effects"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:334
msgid ""
"An important type of history effect is the effect of **repeated testing**. "
"Suppose I want to take two measurements of some psychological construct "
"(e.g., anxiety). One thing I might be worried about is if the first "
"measurement has an effect on the second measurement. In other words, this is"
" a history effect in which the “event” that influences the second "
"measurement is the first measurement itself! This is not at all uncommon. "
"Examples of this include:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:342
msgid ""
"*Learning and practice*: e.g., “intelligence” at time 2 might appear to go "
"up relative to time 1 because participants learned the general rules of how "
"to solve “intelligence-test-style” questions during the first testing "
"session."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:347
msgid ""
"*Familiarity with the testing situation*: e.g., if people are nervous at "
"time 1, this might make performance go down. But after sitting through the "
"first testing situation they might calm down a lot precisely because they’ve"
" seen what the testing looks like."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:352
msgid ""
"*Auxiliary changes caused by testing*: e.g., if a questionnaire assessing "
"mood is boring then mood rating at measurement time 2 is more likely to be "
"“bored” precisely because of the boring measurement made at time 1."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:358
msgid "Selection bias"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:360
msgid ""
"**Selection bias** is a pretty broad term. Suppose that you’re running an "
"experiment with two groups of participants where each group gets a different"
" “treatment”, and you want to see if the different treatments lead to "
"different outcomes. However, suppose that, despite your best efforts, you’ve"
" ended up with a gender imbalance across groups (say, group A has 80% "
"females and group B has 50% females). It might sound like this could never "
"happen but, trust me, it can. This is an example of a selection bias, in "
"which the people “selected into” the two groups have different "
"characteristics. If any of those characteristics turns out to be relevant "
"(say, your treatment works better on females than males) then you’re in a "
"lot of trouble."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:373
msgid "Differential attrition"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:375
msgid ""
"When thinking about the effects of attrition, it is sometimes helpful to "
"distinguish between two different types. The first is **homogeneous "
"attrition**, in which the attrition effect is the same for all groups, "
"treatments or conditions. In the example I gave above, the attrition would "
"be homogeneous if (and only if) the easily bored participants are dropping "
"out of all of the conditions in my experiment at about the same rate. In "
"general, the main effect of homogeneous attrition is likely to be that it "
"makes your sample unrepresentative. As such, the biggest worry that you’ll "
"have is that the generalisability of the results decreases. In other words, "
"you lose external validity."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:386
msgid ""
"The second type of attrition is **heterogeneous attrition**, in which the "
"attrition effect is different for different groups. More often called "
"**differential attrition**, this is a kind of selection bias that is caused "
"by the study itself. Suppose that, for the first time ever in the history of"
" psychology, I manage to find the perfectly balanced and representative "
"sample of people. I start running “Dani’s incredibly long and tedious "
"experiment” on my perfect sample but then, because my study is incredibly "
"long and tedious, lots of people start dropping out. I can’t stop this. "
"Participants absolutely have the right to stop doing any experiment, any "
"time, for whatever reason they feel like, and as researchers we are morally "
"(and professionally) obliged to remind people that they do have this right. "
"So, suppose that “Dani’s incredibly long and tedious experiment” has a very "
"high drop out rate. What do you suppose the odds are that this drop out is "
"random? Answer: zero. Almost certainly the people who remain are more "
"conscientious, more tolerant of boredom, etc., than those that leave. To the"
" extent that (say) conscientiousness is relevant to the psychological "
"phenomenon that I care about, this attrition can decrease the validity of my"
" results."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:405
msgid ""
"Here’s another example. Suppose I design my experiment with two conditions. "
"In the “treatment” condition, the experimenter insults the participant and "
"then gives them a questionnaire designed to measure obedience. In the "
"“control” condition, the experimenter engages in a bit of pointless chitchat"
" and then gives them the questionnaire. Leaving aside the questionable "
"scientific merits and dubious ethics of such a study, let’s have a think "
"about what might go wrong here. As a general rule, when someone insults me "
"to my face I tend to get much less co-operative. So, there’s a pretty good "
"chance that a lot more people are going to drop out of the treatment "
"condition than the control condition. And this drop out isn’t going to be "
"random. The people most likely to drop out would probably be the people who "
"don’t care all that much about the importance of obediently sitting through "
"the experiment. Since the most bloody minded and disobedient people all left"
" the treatment group but not the control group, we’ve introduced a confound:"
" the people who actually took the questionnaire in the treatment group were "
"*already* more likely to be dutiful and obedient than the people in the "
"control group. In short, in this study insulting people doesn’t make them "
"more obedient. It makes the more disobedient people leave the experiment! "
"The internal validity of this experiment is completely shot."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:427
msgid "Non-response bias"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:429
msgid ""
"**Non-response bias** is closely related to selection bias and to "
"differential attrition. The simplest version of the problem goes like this. "
"You mail out a survey to 1000 people but only 300 of them reply. The 300 "
"people who replied are almost certainly not a random subsample. People who "
"respond to surveys are systematically different to people who don’t. This "
"introduces a problem when trying to generalise from those 300 people who "
"replied to the population at large, since you now have a very non-random "
"sample. The issue of non-response bias is more general than this, though. "
"Among the (say) 300 people that did respond to the survey, you might find "
"that not everyone answers every question. If (say) 80 people chose not to "
"answer one of your questions, does this introduce problems? As always, the "
"answer is maybe. If the question that wasn’t answered was on the last page "
"of the questionnaire, and those 80 surveys were returned with the last page "
"missing, there’s a good chance that the missing data isn’t a big deal; "
"probably the pages just fell off. However, if the question that 80 people "
"didn’t answer was the most confrontational or invasive personal question in "
"the questionnaire, then almost certainly you’ve got a problem. In essence, "
"what you’re dealing with here is what’s called the problem of **missing "
"data**. If the data that is missing was “lost” randomly, then it’s not a big"
" problem. If it’s missing systematically, then it can be a big problem."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:452
msgid "Regression to the mean"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:454
msgid ""
"**Regression to the mean** refers to any situation where you select data "
"based on an extreme value on some measure. Because the variable has natural "
"variation it almost certainly means that when you take a subsequent "
"measurement the later measurement will be less extreme than the first one, "
"purely by chance."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:460
msgid ""
"Here’s an example. Suppose I’m interested in whether a psychology education "
"has an adverse effect on very smart kids. To do this, I find the 20 "
"psychology I students with the best high school grades and look at how well "
"they’re doing at university. It turns out that they’re doing a lot better "
"than average, but they’re not topping the class at university even though "
"they did top their classes at high school. What’s going on? The natural "
"first thought is that this must mean that the psychology classes must be "
"having an adverse effect on those students. However, while that might very "
"well be the explanation, it’s more likely that what you’re seeing is an "
"example of “regression to the mean”. To see how it works, let’s take a "
"moment to think about what is required to get the best mark in a class, "
"regardless of whether that class be at high school or at university. When "
"you’ve got a big class there are going to be *lots* of very smart people "
"enrolled. To get the best mark you have to be very smart, work very hard, "
"and be a bit lucky. The exam has to ask just the right questions for your "
"idiosyncratic skills, and you have to avoid making any dumb mistakes (we all"
" do that sometimes) when answering them. And that’s the thing, whilst "
"intelligence and hard work are transferable from one class to the next, luck"
" isn’t. The people who got lucky in high school won’t be the same as the "
"people who get lucky at university. That’s the very definition of “luck”. "
"The consequence of this is that when you select people at the very extreme "
"values of one measurement (the top 20 students), you’re selecting for hard "
"work, skill and luck. But because the luck doesn’t transfer to the second "
"measurement (only the skill and work), these people will all be expected to "
"drop a little bit when you measure them a second time (at university). So "
"their scores fall back a little bit, back towards everyone else. This is "
"regression to the mean."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:489
msgid ""
"Regression to the mean is surprisingly common. For instance, if two very "
"tall people have kids their children will tend to be taller than average but"
" not as tall as the parents. The reverse happens with very short parents. "
"Two very short parents will tend to have short children, but nevertheless "
"those kids will tend to be taller than the parents. It can also be extremely"
" subtle. For instance, there have been studies done that suggested that "
"people learn better from negative feedback than from positive feedback. "
"However, the way that people tried to show this was to give people positive "
"reinforcement whenever they did good, and negative reinforcement when they "
"did bad. And what you see is that after the positive reinforcement people "
"tended to do worse, but after the negative reinforcement they tended to do "
"better. But notice that there’s a selection bias here! When people do very "
"well, you’re selecting for “high” values, and so you should *expect*, "
"because of regression to the mean, that performance on the next trial should"
" be worse regardless of whether reinforcement is given. Similarly, after a "
"bad trial, people will tend to improve all on their own. The apparent "
"superiority of negative feedback is an artefact caused by regression to the "
"mean (see `Kahneman & Tversky, 1973 <References.html#kahneman-1973>`__ for a"
" discussion)."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:511
msgid "Experimenter bias"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:513
msgid ""
"**Experimenter bias** can come in multiple forms. The basic idea is that the"
" experimenter, despite the best of intentions, can accidentally end up "
"influencing the results of the experiment by subtly communicating the “right"
" answer” or the “desired behaviour” to the participants. Typically, this "
"occurs because the experimenter has special knowledge that the participant "
"does not, for example the right answer to the questions being asked or "
"knowledge of the expected pattern of performance for the condition that the "
"participant is in. The classic example of this happening is the case study "
"of “Clever Hans”, which dates back to 1907 (`Pfungst, 1911 "
"<References.html#pfungst-1911>`__; `Hothersall, 2004 "
"<References.html#hothersall-2004>`__\\ ). Clever Hans was a horse that "
"apparently was able to read and count and perform other human like feats of "
"intelligence. After Clever Hans became famous, psychologists started "
"examining his behaviour more closely. It turned out that, not surprisingly, "
"Hans didn’t know how to do maths. Rather, Hans was responding to the human "
"observers around him, because the humans did know how to count and the horse"
" had learned to change its behaviour when people changed theirs."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:532
msgid ""
"The general solution to the problem of experimenter bias is to engage in "
"double blind studies, where neither the experimenter nor the participant "
"knows which condition the participant is in or knows what the desired "
"behaviour is. This provides a very good solution to the problem, but it’s "
"important to recognise that it’s not quite ideal, and hard to pull off "
"perfectly. For instance, the obvious way that I could try to construct a "
"double blind study is to have one of my Ph.D. students (one who doesn’t know"
" anything about the experiment) run the study. That feels like it should be "
"enough. The only person (me) who knows all the details (e.g., correct "
"answers to the questions, assignments of participants to conditions) has no "
"interaction with the participants, and the person who does all the talking "
"to people (the Ph.D. student) doesn’t know anything. Except for the reality "
"that the last part is very unlikely to be true. In order for the Ph.D. "
"student to run the study effectively they need to have been briefed by me, "
"the researcher. And, as it happens, the Ph.D. student also knows me and "
"knows a bit about my general beliefs about people and psychology (e.g., I "
"tend to think humans are much smarter than psychologists give them credit "
"for). As a result of all this, it’s almost impossible for the experimenter "
"to avoid knowing a little bit about what expectations I have. And even a "
"little bit of knowledge can have an effect. Suppose the experimenter "
"accidentally conveys the fact that the participants are expected to do well "
"in this task. Well, there’s a thing called the “Pygmalion effect”, where if "
"you expect great things of people they’ll tend to rise to the occasion. But "
"if you expect them to fail then they’ll do that too. In other words, the "
"expectations become a self-fulfilling prophesy."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:560
msgid "Demand effects and reactivity"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:562
msgid ""
"When talking about experimenter bias, the worry is that the experimenter’s "
"knowledge or desires for the experiment are communicated to the "
"participants, and that these can change people’s behaviour (`Rosenthal, 1966"
" <References.html#rosenthal-1966>`__\\ ). However, even if you manage to "
"stop this from happening, it’s almost impossible to stop people from knowing"
" that they’re part of a psychological study. And the mere fact of knowing "
"that someone is watching or studying you can have a pretty big effect on "
"behaviour. This is generally referred to as **reactivity** or **demand "
"effects**. The basic idea is captured by the Hawthorne effect: people alter "
"their performance because of the attention that the study focuses on them. "
"The effect takes its name from a study that took place in the “Hawthorne "
"Works” factory outside of Chicago (see `Adair, 1984 "
"<References.html#adair-1984>`__\\ ). This study, from the 1920s, looked at "
"the effects of factory lighting on worker productivity. But, importantly, "
"change in worker behaviour occurred because the workers *knew* they were "
"being studied, rather than any effect of factory lighting."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:580
msgid ""
"To get a bit more specific about some of the ways in which the mere fact of "
"being in a study can change how people behave, it helps to think like a "
"social psychologist and look at some of the *roles* that people might "
"*adopt* during an experiment but might *not adopt* if the corresponding "
"events were occurring in the real world:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:586
msgid ""
"The *good participant* tries to be too helpful to the researcher. He or she "
"seeks to figure out the experimenter’s hypotheses and confirm them."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:590
msgid ""
"The *negative participant* does the exact opposite of the good participant. "
"He or she seeks to break or destroy the study or the hypothesis in some way."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:594
msgid ""
"The *faithful participant* is unnaturally obedient. He or she seeks to "
"follow instructions perfectly, regardless of what might have happened in a "
"more realistic setting."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:598
msgid ""
"The *apprehensive participant* gets nervous about being tested or studied, "
"so much so that his or her behaviour becomes highly unnatural, or overly "
"socially desirable."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:603
msgid "Placebo effects"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:605
msgid ""
"The **placebo effect** is a specific type of demand effect that we worry a "
"lot about. It refers to the situation where the mere fact of being treated "
"causes an improvement in outcomes. The classic example comes from clinical "
"trials. If you give people a completely chemically inert drug and tell them "
"that it’s a cure for a disease, they will tend to get better faster than "
"people who aren’t treated at all. In other words, it is people’s belief that"
" they are being treated that causes the improved outcomes, not the drug."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:614
msgid ""
"However, the current consensus in medicine is that true placebo effects are "
"quite rare and most of what was previously considered placebo effect is in "
"fact some combination of natural healing (some people just get better on "
"their own), regression to the mean and other quirks of study design. Of "
"interest to psychology is that the strongest evidence for at least some "
"placebo effect is in self-reported outcomes, most notably in treatment of "
"pain (`Hróbjartsson & Gøtzsche, 2010 "
"<References.html#hrobjartsson-2010>`__\\ )."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:624
msgid "Situation, measurement and sub-population effects"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:626
msgid ""
"In some respects, these terms are a catch-all term for “all other threats to"
" external validity”. They refer to the fact that the choice of sub-"
"population from which you draw your participants, the location, timing and "
"manner in which you run your study (including who collects the data) and the"
" tools that you use to make your measurements might all be influencing the "
"results. Specifically, the worry is that these things might be influencing "
"the results in such a way that the results won’t generalise to a wider array"
" of people, places and measures."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:636
msgid "Fraud, deception and self-deception"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:0
msgid "*It is difficult to get a man to understand something,*"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:0
msgid "*when his salary depends on his not understanding it.*"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:647
msgid "Upton Sinclair"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:645
msgid ""
"There’s one final thing I feel I should mention. While reading what the "
"textbooks often have to say about assessing the validity of a study I "
"couldn’t help but notice that they seem to make the assumption that the "
"researcher is honest. I find this hilarious. While the vast majority of "
"scientists are honest, in my experience at least, some are not.\\ [#]_ Not "
"only that, as I mentioned earlier, scientists are not immune to belief bias."
" It’s easy for a researcher to end up deceiving themselves into believing "
"the wrong thing, and this can lead them to conduct subtly flawed research "
"and then hide those flaws when they write it up. So you need to consider not"
" only the (probably unlikely) possibility of outright fraud, but also the "
"(probably quite common) possibility that the research is unintentionally "
"“slanted”. I opened a few standard textbooks and didn’t find much of a "
"discussion of this problem, so here’s my own attempt to list a few ways in "
"which these issues can arise:"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:661
msgid ""
"**Data fabrication**. Sometimes, people just make up the data. This is "
"occasionally done with “good” intentions. For instance, the researcher "
"believes that the fabricated data do reflect the truth, and may actually "
"reflect “slightly cleaned up” versions of actual data. On other occasions, "
"the fraud is deliberate and malicious. Some high-profile examples where data"
" fabrication has been alleged or shown include Cyril Burt (a psychologist "
"who is thought to have fabricated some of his data), Andrew Wakefield (who "
"has been accused of fabricating his data connecting the MMR vaccine to "
"autism) and Hwang Woo-suk (who falsified a lot of his data on stem cell "
"research)."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:673
msgid ""
"**Hoaxes**. Hoaxes share a lot of similarities with data fabrication, but "
"they differ in the intended purpose. A hoax is often a joke, and many of "
"them are intended to be (eventually) discovered. Often, the point of a hoax "
"is to discredit someone or some field. There’s quite a few well known "
"scientific hoaxes that have occurred over the years (e.g., Piltdown man) and"
" some were deliberate attempts to discredit particular fields of research "
"(e.g., the Sokal affair)."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:681
msgid ""
"**Data misrepresentation**. While fraud gets most of the headlines, it’s "
"much more common in my experience to see data being misrepresented. When I "
"say this I’m not referring to newspapers getting it wrong (which they do, "
"almost always). I’m referring to the fact that often the data don’t actually"
" say what the researchers think they say. My guess is that, almost always, "
"this isn’t the result of deliberate dishonesty but instead is due to a lack "
"of sophistication in the data analyses. For instance, think back to the "
"example of Simpson’s paradox that I discussed in the beginning of this book."
" It’s very common to see people present “aggregated” data of some kind and "
"sometimes, when you dig deeper and find the raw data yourself you find that "
"the aggregated data tell a different story to the disaggregated data. "
"Alternatively, you might find that some aspect of the data is being hidden, "
"because it tells an inconvenient story (e.g., the researcher might choose "
"not to refer to a particular variable). There’s a lot of variants on this, "
"many of which are very hard to detect."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:699
msgid ""
"**Study “misdesign”**. Okay, this one is subtle. Basically, the issue here "
"is that a researcher designs a study that has built-in flaws and those flaws"
" are never reported in the paper. The data that are reported are completely "
"real and are correctly analysed, but they are produced by a study that is "
"actually quite wrongly put together. The researcher really wants to find a "
"particular effect and so the study is set up in such a way as to make it "
"“easy” to (artefactually) observe that effect. One sneaky way to do this, in"
" case you’re feeling like dabbling in a bit of fraud yourself, is to design "
"an experiment in which it’s obvious to the participants what they’re "
"“supposed” to be doing, and then let reactivity work its magic for you. If "
"you want you can add all the trappings of double blind experimentation but "
"it won’t make a difference since the study materials themselves are subtly "
"telling people what you want them to do. When you write up the results the "
"fraud won’t be obvious to the reader. What’s obvious to the participant when"
" they’re in the experimental context isn’t always obvious to the person "
"reading the paper. Of course, the way I’ve described this makes it sound "
"like it’s always fraud. Probably there are cases where this is done "
"deliberately, but in my experience the bigger concern has been with "
"unintentional misdesign. The researcher *believes* and so the study just "
"happens to end up with a built in flaw, and that flaw then magically erases "
"itself when the study is written up for publication."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:723
msgid ""
"**Data mining & post-hoc hypothesising**. Another way in which the authors "
"of a study can more or less misrepresent the data is by engaging in what’s "
"referred to as “data mining” (see `Gelman & Loken, 2014 "
"<References.html#gelman-2014>`__, for a broader discussion of this as part "
"of the “garden of forking paths” in statistical analysis). As we’ll discuss "
"later, if you keep trying to analyse your data in lots of different ways, "
"you’ll eventually find something that “looks” like a real effect but isn’t. "
"This is referred to as “data mining”. It used to be quite rare because data "
"analysis used to take weeks, but now that everyone has very powerful "
"statistical software on their computers it’s becoming very common. Data "
"mining per se isn’t “wrong”, but the more that you do it the bigger the risk"
" you’re taking. The thing that is wrong, and I suspect is very common, is "
"*unacknowledged* data mining. That is, the researcher runs every possible "
"analysis known to humanity, finds the one that works, and then pretends that"
" this was the only analysis that they ever conducted. Worse yet, they often "
"“invent” a hypothesis after looking at the data to cover up the data mining."
" To be clear. It’s not wrong to change your beliefs after looking at the "
"data, and to reanalyse your data using your new “post-hoc” hypotheses. What "
"is wrong (and I suspect common) is failing to acknowledge that you’ve done. "
"If you acknowledge that you did it then other researchers are able to take "
"your behaviour into account. If you don’t, then they can’t. And that makes "
"your behaviour deceptive. Bad!"
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:748
msgid ""
"**Publication bias & self-censoring**. Finally, a pervasive bias is “non-"
"reporting” of negative results. This is almost impossible to prevent. "
"Journals don’t publish every article that is submitted to them. They prefer "
"to publish articles that find “something”. So, if 20 people run an "
"experiment looking at whether reading *Finnegans Wake* causes insanity in "
"humans, and 19 of them find that it doesn’t, which one do you think is going"
" to get published? Obviously, it’s the one study that did find that "
"*Finnegans Wake* causes insanity.\\ [#]_ This is an example of a "
"*publication bias*. Since no-one ever published the 19 studies that didn’t "
"find an effect, a naive reader would never know that they existed. Worse "
"yet, most researchers “internalise” this bias and end up *self-censoring* "
"their research. Knowing that negative results aren’t going to be accepted "
"for publication, they never even try to report them. As a friend of mine "
"says “for every experiment that you get published, you also have 10 "
"failures”. And she’s right. The catch is, while some (maybe most) of those "
"studies are failures for boring reasons (e.g. you stuffed something up) "
"others might be genuine “null” results that you ought to acknowledge when "
"you write up the “good” experiment. And telling which is which is often hard"
" to do. A good place to start is a paper by `Ioannidis (2005) "
"<References.html#ioannidis-2005>`__ with the depressing title “Why most "
"published research findings are false”. I’d also suggest taking a look at "
"work by `Kühberger et al. (2014) <References.html#kuhberger-2014>`__ "
"presenting statistical evidence that this actually happens in psychology."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:774
msgid ""
"There’s probably a lot more issues like this to think about, but that’ll do "
"to start with. What I really want to point out is the blindingly obvious "
"truth that real world science is conducted by actual humans, and only the "
"most gullible of people automatically assumes that everyone else is honest "
"and impartial. Actual scientists aren’t usually *that* naive, but for some "
"reason the world likes to pretend that we are, and the textbooks we usually "
"write seem to reinforce that stereotype."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:785
msgid ""
"The reason why I say that it’s unmeasured is that if you *have* measured it,"
" then you can use some fancy statistical tricks to deal with the confounder."
" Because of the existence of these statistical solutions to the problem of "
"confounders, we often refer to a confounder that we have measured and dealt "
"with as a *covariate*. Dealing with covariates is a more advanced topic, but"
" I thought I’d mention it in passing since it’s kind of comforting to at "
"least know that this stuff exists."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:795
msgid ""
"Some people might argue that if you’re not honest then you’re not a real "
"scientist. Which does have some truth to it I guess, but that’s disingenuous"
" (look up the “No true Scotsman” fallacy). The fact is that there are lots "
"of people who are employed ostensibly as scientists, and whose work has all "
"of the trappings of science, but who are outright fraudulent. Pretending "
"that they don’t exist by saying that they’re not scientists is just muddled "
"thinking."
msgstr ""

#: ../../lsj/Ch02_StudyDesign_5.rst:804
msgid ""
"Clearly, the real effect is that only insane people would even try to read "
"*Finnegans Wake*."
msgstr ""
