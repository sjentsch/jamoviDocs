# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The section authors, The jamovi Group, and Sebastian Jentschke (curating this documentation). This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: jamovi\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-09-10 01:55+0200\n"
"PO-Revision-Date: 2020-08-10 17:57+0000\n"
"Language-Team: Turkish (https://www.transifex.com/jamovi/teams/111618/tr/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: tr\n"
"Plural-Forms: nplurals=2; plural=(n > 1);\n"

#: ../../lsj/Ch14_ANOVA2_06.rst:4
msgid "ANOVA as a linear model"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:6
msgid ""
"One of the most important things to understand about ANOVA and regression is"
" that they’re basically the same thing. On the surface of it, you maybe "
"wouldn’t think this is true. After all, the way that I’ve described them so "
"far suggests that ANOVA is primarily concerned with testing for group "
"differences, and regression is primarily concerned with understanding the "
"correlations between variables. And, as far as it goes that’s perfectly "
"true. But when you look under the hood, so to speak, the underlying "
"mechanics of ANOVA and regression are awfully similar. In fact, if you think"
" about it, you’ve already seen evidence of this. ANOVA and regression both "
"rely heavily on sums of squares (SS), both make use of *F* tests, and so on."
" Looking back, it’s hard to escape the feeling that Chapters `Correlation "
"and linear regression <Ch12_Regression.html#correlation-and-linear-"
"regression>`__ and `Comparing several means (one-way ANOVA) <Ch13_ANOVA.html"
"#comparing-several-means-one-way-anova>`__ were a bit repetitive."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:23
msgid ""
"The reason for this is that ANOVA and regression are both kinds of **linear "
"models**. In the case of regression, this is kind of obvious. The regression"
" equation that we use to define the relationship between predictors and "
"outcomes *is* the equation for a straight line, so it’s quite obviously a "
"linear model, with the equation"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:29 ../../lsj/Ch14_ANOVA2_06.rst:136
msgid ""
"*Y*\\ :sub:`p` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`1p` + *b*\\ "
":sub:`2` *X*\\ :sub:`2p` + ϵ\\ :sub:`p`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:31
msgid ""
"where *Y*\\ :sub:`p` is the outcome value for the *p*-th observation (e.g., "
"*p*-th person), *X*\\ :sub:`1p` is the value of the first predictor for the "
"*p*-th observation, *X*\\ :sub:`2p` is the value of the second predictor for"
" the *p*-th observation, the *b*\\ :sub:`0`, *b*\\ :sub:`1`, and *b*\\ "
":sub:`2` terms are our regression coefficients, and *ϵ*\\ :sub:`p` is the "
"*p*-th residual. If we ignore the residuals *ϵ*\\ :sub:`p` and just focus on"
" the regression line itself, we get the following formula:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:40
msgid ""
"*Ŷ*\\ :sub:`p` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`1p` + *b*\\ "
":sub:`2` *X*\\ :sub:`2p`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:42
msgid ""
"where *Ŷ*\\ :sub:`p` is the value of *Y* that the regression line predicts "
"for person *p*, as opposed to the actually-observed value *Y*\\ :sub:`p`. "
"The thing that isn’t immediately obvious is that we can write ANOVA as a "
"linear model as well. However, it’s actually pretty straightforward to do "
"this. Let’s start with a really simple example, rewriting a 2 × 2 factorial "
"ANOVA as a linear model."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:50
msgid "Some data"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:52
msgid ""
"To make things concrete, let’s suppose that our outcome variable is the "
"``grade`` that a student receives in my class, a ratio-scale variable "
"corresponding to a mark from 0% to 100%. There are two predictor variables "
"of interest: whether or not the student turned up to lectures (the "
"``attend`` variable) and whether or not the student actually read the "
"textbook (the ``reading`` variable). We’ll say that ``attend = 1`` if the "
"student attended class, and ``attend = 0`` if they did not. Similarly, we’ll"
" say that ``reading = 1`` if the student read the textbook, and ``reading = "
"0`` if they did not."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:62
msgid ""
"Okay, so far that’s simple enough. The next thing we need to do is to wrap "
"some maths around this (sorry!). For the purposes of this example, let *Y*\\"
" :sub:`p` denote the ``grade`` of the *p*-th student in the class. This is "
"not quite the same notation that we used earlier in this chapter. "
"Previously, we’ve used the notation *Y*\\ :sub:`rci` to refer to the i-th "
"person in the *r*-th group for predictor 1 (the row factor) and the *c*-th "
"group for predictor 2 (the column factor). This extended notation was really"
" handy for describing how the SS values are calculated, but it’s a pain in "
"the current context, so I’ll switch notation here. Now, the *Y*\\ :sub:`p` "
"notation is visually simpler than *Y*\\ :sub:`rci`, but it has the "
"shortcoming that it doesn’t actually keep track of the group memberships! "
"That is, if I told you that *Y*\\ :sub:`0,0,3` = 35, you’d immediately know "
"that we’re talking about a student (the 3rd such student, in fact) who "
"didn’t attend the lectures (i.e., ``attend = 0``) and didn’t read the "
"textbook (i.e. ``reading = 0``), and who ended up failing the class (``grade"
" = 35``). But if I tell you that *Y*\\ :sub:`p` = 35, all you know is that "
"the *p*-th student didn’t get a good grade. We’ve lost some key information "
"here. Of course, it doesn’t take a lot of thought to figure out how to fix "
"this. What we’ll do instead is introduce two new variables *X*\\ :sub:`1p` "
"and *X*\\ :sub:`2p` that keep track of this information. In the case of our "
"hypothetical student, we know that *X*\\ :sub:`1p` = 0 (i.e., ``attend = "
"0``) and *X*\\ :sub:`2p` = 0 (i.e., ``reading = 0``). So the data might look"
" like this:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:88
msgid "person, p"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:88
msgid "grade, *Y*\\ :sub:`p`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:88
msgid "attendance, *X*\\ :sub:`1p`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:88
msgid "reading, *X*\\ :sub:`2p`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:91 ../../lsj/Ch14_ANOVA2_06.rst:91
#: ../../lsj/Ch14_ANOVA2_06.rst:91 ../../lsj/Ch14_ANOVA2_06.rst:93
#: ../../lsj/Ch14_ANOVA2_06.rst:93 ../../lsj/Ch14_ANOVA2_06.rst:95
#: ../../lsj/Ch14_ANOVA2_06.rst:97 ../../lsj/Ch14_ANOVA2_06.rst:103
#: ../../lsj/Ch14_ANOVA2_06.rst:105 ../../lsj/Ch14_ANOVA2_06.rst:342
#: ../../lsj/Ch14_ANOVA2_06.rst:343
msgid "1"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:91
msgid "90"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:93
msgid "2"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:93
msgid "87"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:95
msgid "3"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:95
msgid "75"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:95 ../../lsj/Ch14_ANOVA2_06.rst:97
#: ../../lsj/Ch14_ANOVA2_06.rst:99 ../../lsj/Ch14_ANOVA2_06.rst:99
#: ../../lsj/Ch14_ANOVA2_06.rst:101 ../../lsj/Ch14_ANOVA2_06.rst:101
#: ../../lsj/Ch14_ANOVA2_06.rst:103 ../../lsj/Ch14_ANOVA2_06.rst:105
#: ../../lsj/Ch14_ANOVA2_06.rst:341 ../../lsj/Ch14_ANOVA2_06.rst:341
#: ../../lsj/Ch14_ANOVA2_06.rst:342 ../../lsj/Ch14_ANOVA2_06.rst:343
msgid "0"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:97
msgid "4"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:97
msgid "60"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:99
msgid "5"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:99
msgid "35"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:101
msgid "6"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:101
msgid "50"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:103
msgid "7"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:103
msgid "65"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:105
msgid "8"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:105
msgid "70"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:108
msgid ""
"This isn’t anything particularly special, of course. It’s exactly the format"
" in which we expect to see our data! See the ``rtfm`` data set. We can use "
"the jamovi ‘Descriptives’ analysis to confirm that this data set corresponds"
" to a balanced design, with 2 observations for each combination of "
"``attend`` and ``reading``. In the same way we can also calculate the mean "
"grade for each combination. This is shown in :numref:`fig-rtfmdescriptives`."
" Looking at the mean scores, one gets the strong impression that reading the"
" text and attending the class both matter a lot."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:123
msgid "jamovi descriptives for the ``rtfm`` dataset"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:128
msgid "ANOVA with binary factors as a regression model"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:130
msgid ""
"Okay, let’s get back to talking about the mathematics. We now have our data "
"expressed in terms of three numeric variables: the continuous variable *Y* "
"and the two binary variables *X*\\ :sub:`1` and *X*\\ :sub:`2`. What I want "
"you to recognise is that our 2 × 2 factorial ANOVA is *exactly* equivalent "
"to the regression model:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:138
msgid ""
"This is, of course, the exact same equation that I used earlier to describe "
"a two-predictor regression model! The only difference is that *X*\\ :sub:`1`"
" and *X*\\ :sub:`2` are now *binary* variables (i.e., values can only be 0 "
"or 1), whereas in a regression analysis we expect that *X*\\ :sub:`1` and "
"*X*\\ :sub:`2` will be continuous. There’s a couple of ways I could try to "
"convince you of this. One possibility would be to do a lengthy mathematical "
"exercise proving that the two are identical. However, I’m going to go out on"
" a limb and guess that most of the readership of this book will find that "
"annoying rather than helpful. Instead, I’ll explain the basic ideas and then"
" rely on jamovi to show that ANOVA analyses and regression analyses aren’t "
"just similar, they’re identical for all intents and purposes. Let’s start by"
" running this as an ANOVA. To do this, we’ll use the ``rtfm`` data set, and "
":numref:`fig-factorialanova6` shows what we get when we run the analysis in "
"jamovi."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:159
msgid ""
"ANOVA of the ``rtfm`` dataset in jamovi: Model with two factors ``attend`` "
"and ``reading`` but without the interaction term for these two factors"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:164
msgid ""
"So, by reading the key numbers off the ANOVA table and the mean scores that "
"we presented earlier, we can see that the students obtained a higher grade "
"if they attended class (*F*\\(1,5) = 21.6, p = 0.0056) and if they read the "
"textbook: *F*\\(1,5) = 52.3, p = 0.0008. Let’s make a note of those "
"*p*-values and those *F* statistics."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:170
msgid ""
"Now let’s think about the same analysis from a linear regression "
"perspective. In the ``rtfm`` data set, we have encoded ``attend`` and "
"``reading`` as if they were numeric predictors. In this case, this is "
"perfectly acceptable. There really is a sense in which a student who turns "
"up to class (i.e. ``attend = 1``) has in fact done “more attendance” than a "
"student who does not (i.e. ``attend = 0``). So it’s not at all unreasonable "
"to include it as a predictor in a regression model. It’s a little unusual, "
"because the predictor only takes on two possible values, but it doesn’t "
"violate any of the assumptions of linear regression. And it’s easy to "
"interpret. If the regression coefficient for ``attend`` is greater than 0 it"
" means that students that attend lectures get higher grades. If it’s less "
"than zero then students attending lectures get lower grades. The same is "
"true for our ``reading`` variable."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:183
msgid ""
"Wait a second though. *Why* is this true? It’s something that is intuitively"
" obvious to everyone who has taken a few stats classes and is comfortable "
"with the maths, but it *isn’t* clear to everyone else at first pass. To see "
"why this is true, it helps to look closely at a few specific students. Let’s"
" start by considering the 6th and 7th students in our data set (i.e. p = 6 "
"and p = 7). Neither one has read the textbook, so in both cases we can set "
"``reading = 0``. Or, to say the same thing in our mathematical notation, we "
"observe *X*\\ :sub:`2,6` = 0 and *X*\\ :sub:`2,7` = 0. However, student "
"number 7 did turn up to lectures (i.e., ``attend = 1``, *X*\\ :sub:`1,7` = "
"1) whereas student number 6 did not (i.e., ``attend = 0``, *X*\\ :sub:`1,6` "
"= 0). Now let’s look at what happens when we insert these numbers into the "
"general formula for our regression line. For student number 6, the "
"regression predicts that:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:196
msgid ""
"*Ŷ*\\ :sub:`6` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`1,6` +  *b*\\ "
":sub:`2` *X*\\ :sub:`2,6`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:197
msgid "*Ŷ*\\ :sub:`6` = *b*\\ :sub:`0` + *b*\\ :sub:`1` × 0 + *b*\\ :sub:`2` × 0"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:198
msgid "*Ŷ*\\ :sub:`6` = *b*\\ :sub:`0`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:200
msgid ""
"So we’re expecting that this student will obtain a grade corresponding to "
"the value of the intercept term *b*\\ :sub:`0`. What about student 7? This "
"time when we insert the numbers into the formula for the regression line, we"
" obtain the following:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:205
msgid ""
"*Ŷ*\\ :sub:`7` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`1,7` +  *b*\\ "
":sub:`2` *X*\\ :sub:`2,7`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:206
msgid "*Ŷ*\\ :sub:`7` = *b*\\ :sub:`0` + *b*\\ :sub:`1` × 1 + *b*\\ :sub:`2` × 0"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:207
msgid "*Ŷ*\\ :sub:`7` = *b*\\ :sub:`0` + *b*\\ :sub:`1`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:209
msgid ""
"Because this student attended class, the predicted grade is equal to the "
"intercept term *b*\\ :sub:`0` *plus* the coefficient associated with the "
"``attend`` variable, *b*\\ :sub:`1`. So, if *b*\\ :sub:`1` is greater than "
"zero, we’re expecting that the students who turn up to lectures will get "
"higher grades than those students who don’t. If this coefficient is negative"
" we’re expecting the opposite: students who turn up at class end up "
"performing much worse. In fact, we can push this a little bit further. What "
"about student number 1, who turned up to class (*X*\\ :sub:`1,1` = 1) *and* "
"read the textbook (*X*\\ :sub:`2,1` = 1)? If we plug these numbers into the "
"regression we get:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:219
msgid ""
"*Ŷ*\\ :sub:`1` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`1,1` +  *b*\\ "
":sub:`2` *X*\\ :sub:`2,1`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:220
msgid "*Ŷ*\\ :sub:`1` = *b*\\ :sub:`0` + *b*\\ :sub:`1` × 1 + *b*\\ :sub:`2` × 1"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:221
msgid "*Ŷ*\\ :sub:`1` = *b*\\ :sub:`0` + *b*\\ :sub:`1` + *b*\\ :sub:`2`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:223
msgid ""
"So if we assume that attending class helps you get a good grade (i.e., *b*\\"
" :sub:`1 > 0`) and if we assume that reading the textbook also helps you get"
" a good grade (i.e., *b*\\ :sub:`2 >0`), then our expectation is that "
"student 1 will get a grade that that is higher than student 6 and student 7."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:228
msgid ""
"And at this point you won’t be at all suprised to learn that the regression "
"model predicts that student 3, who read the book but didn’t attend lectures,"
" will obtain a grade of *b*\\ :sub:`2 + b_0`. I won’t bore you with yet "
"another regression formula. Instead, what I’ll do is show you the following "
"table of *expected grades*:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:237
msgid "attended?"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:237
msgid "no yes"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:237
msgid "*b*\\ :sub:`0` *b*\\ :sub:`0 + b_1`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:237
msgid "*b*\\ :sub:`0 + b_2` *b*\\ :sub:`0 + b_1 + b_2`"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:241
msgid ""
"As you can see, the intercept term *b*\\ :sub:`0` acts like a kind of "
"“baseline” grade that you would expect from those students who don’t take "
"the time to attend class or read the textbook. Similarly, *b*\\ :sub:`1` "
"represents the boost that you’re expected to get if you come to class, and "
"*b*\\ :sub:`2` represents the boost that comes from reading the textbook. In"
" fact, if this were an ANOVA you might very well want to characterise *b*\\ "
":sub:`1` as the main effect of attendance, and *b*\\ :sub:`2` as the main "
"effect of reading! In fact, for a simple 2 × 2 ANOVA that’s *exactly* how it"
" plays out."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:250
msgid ""
"Okay, now that we’re really starting to see why ANOVA and regression are "
"basically the same thing, let’s actually run our regression using the "
"``rtfm`` data set and the jamovi regression analysis to convince ourselves "
"that this is really true. Running the regression in the usual way gives the "
"results shown in :numref:`fig-factorialanova7`."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:262
msgid ""
"Regression analysis for the ``rtfm`` dataset in jamovi: Model with two "
"factors ``attend`` and ``reading`` but without the interaction term for "
"these two factors"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:268
msgid ""
"There’s a few interesting things to note here. First, notice that the "
"intercept term is 43.5 which is close to the “group” mean of 42.5 observed "
"for those two students who didn’t read the text or attend class. Second, "
"notice that we have the regression coefficient of *b*\\ :sub:`1` = 18.0 for "
"the attendance variable, suggesting that those students that attended class "
"scored 18% higher than those who didn’t. So our expectation would be that "
"those students who turned up to class but didn’t read the textbook would "
"obtain a grade of *b*\\ :sub:`0` + *b*\\ :sub:`1`, which is equal to 43.5 + "
"18.0 = 61.5. You can verify for yourself that the same thing happens when we"
" look at the students that read the textbook."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:279
msgid ""
"Actually, we can push a little further in establishing the equivalence of "
"our ANOVA and our regression. Look at the *p*-values associated with the "
"``attend`` variable and the ``reading`` variable in the regression output. "
"They’re identical to the ones we encountered earlier when running the ANOVA."
" This might seem a little surprising, since the test used when running our "
"regression model calculates a *t*-statistic and the ANOVA calculates an "
"*F*-statistic. However, if you can remember all the way back to Chapter "
"`Introduction to probability <Ch07_Probability.html#introduction-to-"
"probability>`__, I mentioned that there’s a relationship between the "
"*t*-distribution and the *F*-distribution. If you have some quantity that is"
" distributed according to a *t*-distribution with k degrees of freedom and "
"you square it, then this new squared quantity follows an *F*-distribution "
"whose degrees of freedom are 1 and k. We can check this with respect to the "
"*t*-statistics in our regression model. For the ``attend`` variable we get a"
" *t*-value of 4.65. If we square this number we end up with 21.6, which "
"matches the corresponding *F* statistic in our ANOVA."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:295
msgid ""
"Finally, one last thing you should know. Because jamovi understands the fact"
" that ANOVA and regression are both examples of linear models, it lets you "
"extract the classic ANOVA table from your regression model using the "
"``Linear Regression`` - ``Model Coefficients`` - ``Omnibus Test`` - ``ANOVA "
"Test``, and this will give you the table shown in :numref:`fig-"
"factorialanova8`."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:307
msgid ""
"Results table showing the Omnibus ANOVA Test from the jamovi regression "
"analysis using the ``rtfm`` dataset"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:313
msgid "How to encode non binary factors as contrasts"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:315
msgid ""
"At this point, I’ve shown you how we can view a 2 × 2 ANOVA into a linear "
"model. And it’s pretty easy to see how this generalises to a 2 × 2 × 2 ANOVA"
" or a 2 × 2 × 2 × 2 ANOVA. It’s the same thing, really. You just add a new "
"binary variable for each of your factors. Where it begins to get trickier is"
" when we consider factors that have more than two levels. Consider, for "
"instance, the 3 × 2 ANOVA that we ran earlier in this chapter using the "
"``clinicaltrial`` data set. How can we convert the three-level ``drug`` "
"factor into a numerical form that is appropriate for a regression?"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:324
msgid ""
"The answer to this question is pretty simple, actually. All we have to do is"
" realise that a three-level factor can be redescribed as *two* binary "
"variables. Suppose, for instance, I were to create a new binary variable "
"called ``druganxifree``. Whenever the ``drug`` variable is equal to "
"``\"anxifree\"`` we set ``druganxifree = 1``. Otherwise, we set "
"``druganxifree = 0``. This variable sets up a **contrast**, in this case "
"between anxifree and the other two drugs. By itself, of course, the "
"``druganxifree`` contrast isn’t enough to fully capture all of the "
"information in our ``drug`` variable. We need a second contrast, one that "
"allows us to distinguish between joyzepam and the placebo. To do this, we "
"can create a second binary contrast, called ``drugjoyzepam``, which equals 1"
" if the drug is joyzepam and 0 if it is not. Taken together, these two "
"contrasts allows us to perfectly discriminate between all three possible "
"drugs. The table below illustrates this:"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:339
msgid "``drug``"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:339
msgid "``druganxifree``"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:339
msgid "``drugjoyzepam``"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:341
msgid "``\"placebo\"``"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:342
msgid "``\"anxifree\"``"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:343
msgid "``\"joyzepam\"``"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:346
msgid ""
"If the drug administered to a patient is a placebo then both of the two "
"contrast variables will equal 0. If the drug is Anxifree then the "
"``druganxifree`` variable will equal 1, and ``drugjoyzepam`` will be 0. The "
"reverse is true for Joyzepam: ``drugjoyzepam`` is 1 and ``druganxifree`` is "
"0."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:351
msgid ""
"Creating contrast variables is not too difficult to do using the jamovi "
"``Compute`` command to create a new variable. For example, to create the "
"``druganxifree`` variable, write this logical expression in the formula box:"
" ``IF(drug == ’anxifree’, 1, 0)``. Similarly, to create the new variable "
"``drugjoyzepam`` use this logical expression: ``IF(drug == ’joyzepam’, 1, "
"0)``. Likewise for ``CBTtherapy``: ``IF(therapy == ’CBT’, 1, 0)``. You can "
"see these new variables, and the corresponding logical expressions, in the "
"``clinicaltrial2`` data set."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:360
msgid ""
"We have now recoded our three-level factor in terms of two binary variables,"
" and we’ve already seen that ANOVA and regression behave the same way for "
"binary variables. However, there are some additional complexities that arise"
" in this case, which we’ll discuss in the next section."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:366
msgid "The equivalence between ANOVA and regression for non-binary factors"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:368
msgid ""
"Now we have two different versions of the same data set. Our original data "
"in which the ``drug`` variable from the ``clinicaltrial`` data set is "
"expressed as a single three-level factor, and the ``clinicaltrial2`` data "
"set in which it is expanded into two binary contrasts. Once again, the thing"
" that we want to demonstrate is that our original 3 × 2 factorial ANOVA is "
"equivalent to a regression model applied to the contrast variables. Let’s "
"start by re-running the ANOVA, with results shown in :numref:`fig-"
"factorialanova9`."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:382
msgid ""
"jamovi ANOVA results for the ``clinicaltrial`` dataset: Unsaturated model "
"with the two main effects for ``drug`` and ``therapy`` but without an "
"interaction component for these two factors"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:388
msgid ""
"Obviously, there are no surprises here. That’s the exact same ANOVA that we "
"ran earlier. Next, let’s run a regression using ``druganxifree``, "
"``drugjoyzepam`` and ``CBTtherapy`` as the predictors. The results are shown"
" in :numref:`fig-factorialanova10`."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:399
msgid ""
"jamovi regression results for the ``clinicaltrial`` dataset: Model with the "
"generated contrast variables ``druganxifree`` and ``drugjoyzepam``"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:404
msgid ""
"Hmm. This isn’t the same output that we got last time. Not surprisingly, the"
" regression output prints out the results for each of the three predictors "
"separately, just like it did every other time we conducted a regression "
"analysis. On the one hand we can see that the *p*-value for the "
"``CBTtherapy`` variable is exactly the same as the one for the ``therapy`` "
"factor in our original ANOVA, so we can be reassured that the regression "
"model is doing the same thing as the ANOVA did. On the other hand, this "
"regression model is testing the ``druganxifree`` contrast and the "
"``drugjoyzepam`` contrast *separately*, as if they were two completely "
"unrelated variables. It’s not surprising of course, because the poor "
"regression analysis has no way of knowing that ``drugjoyzepam`` and "
"``druganxifree`` are actually the two different contrasts that we used to "
"encode our three-level ``drug`` factor. As far as it knows, ``drugjoyzepam``"
" and ``druganxifree`` are no more related to one another than "
"``drugjoyzepam`` and ``therapyCBT``. However, you and I know better. At this"
" stage we’re not at all interested in determining whether these two "
"contrasts are individually significant. We just want to know if there’s an "
"“overall” effect of drug. That is, what *we* want jamovi to do is to run "
"some kind of “model comparison” test, one in which the two “drug-related” "
"contrasts are lumped together for the purpose of the test. Sound familiar? "
"All we need to do is specify our null model, which in this case would "
"include the ``CBTtherapy`` predictor, and omit both of the drug-related "
"variables, as in :numref:`fig-factorialanova11`\\."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:433
msgid ""
"Model comparison in jamovi regression: Null model (Model 1) vs. model using "
"the generated contrast variables (Model 2)"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:438
msgid ""
"Ah, that’s better. Our *F*-statistic is 26.15, the degrees of freedom are 2 "
"and 14, and the *p*-value is 0.00002. The numbers are identical to the ones "
"we obtained for the main effect of ``drug`` in our original ANOVA. Once "
"again we see that ANOVA and regression are essentially the same. They are "
"both linear models, and the underlying statistical machinery for ANOVA is "
"identical to the machinery used in regression. The importance of this fact "
"should not be understated. Throughout the rest of this chapter we’re going "
"to rely heavily on this idea."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:447
msgid ""
"Although we went through all the faff of computing new variables in jamovi "
"for the contrasts ``druganxifree`` and ``drugjoyzepam``, just to show that "
"ANOVA and regression are essentially the same, in the jamovi linear "
"regression analysis there is actually a nifty shortcut to get these "
"contrasts, see :numref:`fig-regfactors`. What jamovi is doing here is "
"allowing you to enter the predictor variables that are factors as, wait for "
"it...factors! Smart, eh. You can also specify which group to use as the "
"reference level, via the ``Reference Levels`` option. We’ve changed this to "
"``placebo`` and ``no.therapy``, respectively, because this makes most sense."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:463
msgid ""
"Regression analysis with factors and contrasts in jamovi, including omnibus "
"ANOVA test results"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:468
msgid ""
"If you also click on the ``ANOVA`` test checkbox under the ``Model "
"Coefficients`` → ``Omnibus Test`` option, we see that the *F*-statistic is "
"26.15, the degrees of freedom are 2 and 14, and the *p*-value is 0.00002 "
"(see :numref:`fig-regfactors`). The numbers are identical to the ones we "
"obtained for the main effect of ``drug`` in our original ANOVA. Once again, "
"we see that ANOVA and regression are essentially the same. They are both "
"linear models, and the underlying statistical machinery for ANOVA is "
"identical to the machinery used in regression."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:478
msgid "Degrees of freedom as parameter counting!"
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:480
msgid ""
"At long last, I can finally give a definition of degrees of freedom that I "
"am happy with. Degrees of freedom are defined in terms of the number of "
"parameters that have to be estimated in a model. For a regression model or "
"an ANOVA, the number of parameters corresponds to the number of regression "
"coefficients (i.e. *b*-values), including the intercept. Keeping in mind "
"that any *F*-test is always a comparison between two models and the first "
"*df* is the difference in the number of parameters. For example, in the "
"model comparison above, the null model (``mood.gain ~ therapyCBT``) has two "
"parameters: there’s one regression coefficient for the ``therapyCBT`` "
"variable, and a second one for the intercept. The alternative model "
"(``mood.gain ~ druganxifree + drugjoyzepam + therapyCBT``) has four "
"parameters: one regression coefficient for each of the three contrasts, and "
"one more for the intercept. So the degrees of freedom associated with the "
"*difference* between these two models is *df*\\ :sub:`1` = 4 - 2 = 2."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:496
msgid ""
"What about the case when there doesn’t seem to *be* a null model? For "
"instance, you might be thinking of the *F*-test that shows up when you "
"select ‘\\ *F* Test‘ under the ‘Linear Regression’ - ‘Model Fit’ options. I "
"originally described that as a test of the regression model as a whole. "
"However, that is still a comparison between two models. The null model is "
"the trivial model that only includes 1 regression coefficient, for the "
"intercept term. The alternative model contains K + 1 regression "
"coefficients, one for each of the K predictor variables and one more for the"
" intercept. So the *df*-value that you see in this *F* test is equal to "
"*df*\\ :sub:`1` = K + 1 - 1 = K."
msgstr ""

#: ../../lsj/Ch14_ANOVA2_06.rst:508
msgid ""
"What about the second *df*-value that appears in the *F*-test? This always "
"refers to the degrees of freedom associated with the residuals. It is "
"possible to think of this in terms of parameters too, but in a slightly "
"counter-intuitive way. Think of it like this. Suppose that the total number "
"of observations across the study as a whole is *N*. If you wanted to "
"*perfectly* describe each of these *N* values, you need to do so using, "
"well... *N* numbers. When you build a regression model, what you’re really "
"doing is specifying that some of the numbers need to perfectly describe the "
"data. If your model has K predictors and an intercept, then you’ve specified"
" K + 1 numbers. So, without bothering to figure out exactly *how* this would"
" be done, how many *more* numbers do you think are going to be needed to "
"transform a K + 1 parameter regression model into a perfect re-description "
"of the raw data? If you found yourself thinking that (K + 1) + (N - K - 1) ="
" N, and so the answer would have to be N - K - 1, well done! That’s exactly "
"right. In principle you can imagine an absurdly complicated regression model"
" that includes a parameter for every single data point, and it would of "
"course provide a perfect description of the data. This model would contain "
"*N* parameters in total, but we’re interested in the difference between the "
"number of parameters required to describe this full model (i.e. *N*) and the"
" number of parameters used by the simpler regression model that you’re "
"actually interested in (i.e., K + 1), and so the second degrees of freedom "
"in the *F* test is *df*\\ :sub:`2` = N - K - 1, where K is the number of "
"predictors (in a regression model) or the number of contrasts (in an ANOVA)."
" In the example I gave above, there are *N* = 18 observations in the data "
"set and K + 1 = 4 regression coefficients associated with the ANOVA model, "
"so the degrees of freedom for the residuals is *df*\\ :sub:`2` = 18 - 4 = "
"14."
msgstr ""
