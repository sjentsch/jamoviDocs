# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The section authors, The jamovi Group, and Sebastian
# Jentschke (curating this documentation). This work is licensed under a
# Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: jamovi \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-08-27 11:03+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../lsj/Ch11_tTest_09.rst:4
msgid "Testing non-normal data with Wilcoxon tests"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:6
msgid ""
"Okay, suppose your data turn out to be pretty substantially non-normal, but "
"you still want to run something like a *t*-test? This situation occurs a lot "
"in real life. For the AFL winning margins data, for instance, the Shapiro-"
"Wilk test made it very clear that the normality assumption is violated. This "
"is the situation where you want to use Wilcoxon tests."
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:13
msgid ""
"Like the *t*-test, the Wilcoxon test comes in two forms, one-sample and two-"
"sample, and they’re used in more or less the exact same situations as the "
"corresponding *t*-tests. Unlike the *t*-test, the Wilcoxon test doesn’t "
"assume normality, which is nice. In fact, they don’t make any assumptions "
"about what kind of distribution is involved. In statistical jargon, this "
"makes them **nonparametric tests**. While avoiding the normality assumption "
"is nice, there’s a drawback: the Wilcoxon test is usually less powerful than "
"the *t*-test (i.e., higher Type II error rate). I won’t discuss the Wilcoxon "
"tests in as much detail as the *t*-tests, but I’ll give you a brief overview."
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:26
msgid "Two sample Mann-Whitney U test"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:28
msgid ""
"I’ll start by describing the **Mann-Whitney U test**, since it’s actually "
"simpler than the one sample version. Suppose we’re looking at the scores of "
"10 people on some test. Since my imagination has now failed me completely, "
"let’s pretend it’s a “test of awesomeness” and there are two groups of "
"people, “A” and “B”. I’m curious to know which group is more awesome. The "
"data are included in the ``awesome`` dataset, and there are two variables "
"apart from the usual ``ID`` variable: ``scores`` and ``group``."
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:37
msgid ""
"As long as there are no ties (i.e., people with the exact same awesomeness "
"score) then the test that we want to do is surprisingly simple. All we have "
"to do is construct a table that compares every observation in group A against "
"every observation in group B. Whenever the group A datum is larger, we place "
"a check mark in the table:"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:44
msgid "Group B"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:46
msgid "14.5"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:46
msgid "10.4"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:46
msgid "12.4"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:46
msgid "11.7"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:46
msgid "13.0"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:52
msgid "Group A"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:48
msgid "6.4"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:50
msgid "10.7"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:50 ../../lsj/Ch11_tTest_09.rst:52
#: ../../lsj/Ch11_tTest_09.rst:52 ../../lsj/Ch11_tTest_09.rst:95
#: ../../lsj/Ch11_tTest_09.rst:95 ../../lsj/Ch11_tTest_09.rst:95
#: ../../lsj/Ch11_tTest_09.rst:97
msgid "✓"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:52
msgid "11.9"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:54
msgid "7.3"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:56
msgid "10.0"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:59
msgid ""
"We then count up the number of checkmarks. This is our test statistic, *W*.\\ "
"[#]_ The actual sampling distribution for *W* is somewhat complicated, and "
"I’ll skip the details. For our purposes, it’s sufficient to note that the "
"interpretation of *W* is qualitatively the same as the interpretation of *t* "
"or *z*. That is, if we want a two-sided test then we reject the null "
"hypothesis when *W* is very large or very small, but if we have a directional "
"(i.e., one-sided) hypothesis then we only use one or the other."
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:67
msgid ""
"In jamovi, if we run an ``Independent Samples t-Test`` with ``scores`` as the "
"dependent variable. and ``group`` as the grouping variable, and then under "
"the options for ``Tests`` check the option for ``Mann-Whitney U``, we will "
"get results showing that U = 3 (i.e., the same number of check marks as shown "
"above), and a p-value = 0.05556."
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:74
msgid "One sample Wilcoxon test"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:76
msgid ""
"What about the **one sample Wilcoxon test** (or equivalently, the paired "
"samples Wilcoxon test)? Suppose I’m interested in finding out whether taking "
"a statistics class has any effect on the happiness of students. My data is in "
"the ``happiness.csv`` file. What I’ve measured here is the happiness of each "
"student ``before`` taking the class and ``after`` taking the class, and the "
"``change`` score is the difference between the two. Just like we saw with the "
"*t*-test, there’s no fundamental difference between doing a paired-samples "
"test using ``before`` and ``after``, versus doing a one-sample test using the "
"``change`` scores. As before, the simplest way to think about the test is to "
"construct a tabulation. The way to do it this time is to take those change "
"scores that are positive differences, and tabulate them against all the "
"complete sample. What you end up with is a table that looks like this:"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:91
msgid "all differences"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "-24"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "-14"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "-10"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "7"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "-6"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "-38"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "2"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "-35"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "-30"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:93
msgid "5"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:96
msgid "positive differences"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:95
msgid "7 2 5"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:95
msgid "✓ ✓ ✓"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:100
msgid ""
"Counting up the tick marks this time we get a test statistic of *W* = 7. As "
"before, if our test is two-sided, then we reject the null hypothesis when *W* "
"is very large or very small. As far as running it in jamovi goes, it’s pretty "
"much what you’d expect. For the one-sample version, you specify the "
"``Wilcoxon rank`` option under ``Tests`` in the ``One Sample *t*-Test`` "
"analysis panel.This gives you Wilcoxon *W* = 7, *p*-value = 0.03711. As this "
"shows, we have a significant effect. Evidently, taking a statistics class "
"does have an effect on your happiness. Switching to a paired samples version "
"of the test won’t give us a different answer, of course; see :numref:`fig-"
"ttest_nonparametric`."
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:117
msgid ""
"jamovi screen showing results for one sample and paired sample Wilcoxon non-"
"parametric tests"
msgstr ""

#: ../../lsj/Ch11_tTest_09.rst:125
msgid ""
"Actually, there are two different versions of the test statistic that differ "
"from each other by a constant value. The version that I’ve described is the "
"one that jamovi calculates."
msgstr ""

