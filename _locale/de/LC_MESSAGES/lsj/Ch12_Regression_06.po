# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The section authors, The jamovi Group, and Sebastian Jentschke (curating this documentation). This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: jamovi\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-08-14 18:31+0200\n"
"PO-Revision-Date: 2020-08-10 17:56+0000\n"
"Language-Team: German (https://www.transifex.com/jamovi/teams/111618/de/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: de\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"

#: ../../lsj/Ch12_Regression_06.rst:4
msgid "Quantifying the fit of the regression model"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:6
msgid ""
"So we now know how to estimate the coefficients of a linear regression "
"model. The problem is, we don’t yet know if this regression model is any "
"good. For example, the ``regression.1`` model *claims* that every hour of "
"sleep will improve my mood by quite a lot, but it might just be rubbish. "
"Remember, the regression model only produces a prediction *Ŷ*\\ :sub:`i` "
"about what my mood is like, but my actual mood is *Y*\\ :sub:`i`. If these "
"two are very close, then the regression model has done a good job. If they "
"are very different, then it has done a bad job."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:16
msgid "The *R²* (R-squared) value"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:18
msgid ""
"Once again, let’s wrap a little bit of mathematics around this. Firstly, "
"we’ve got the sum of the squared residuals"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:21
msgid "\\mbox{SS}_{res} = \\sum_i (Y_i - \\hat{Y}_i)^2"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:23
msgid ""
"which we would hope to be pretty small. Specifically, what we’d like is for "
"it to be very small in comparison to the total variability in the outcome "
"variable"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:27
msgid "\\mbox{SS}_{tot} = \\sum_i (Y_i - \\bar{Y})^2"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:29
msgid ""
"While we’re here, let’s calculate these values ourselves, not by hand "
"though. Let’s use something like Excel or another standard spreadsheet "
"programme. I have done this by opening up the ``parenthood.csv`` file in "
"Excel and saving it as ``parenthood_rsquared.xls`` so that I can work on it."
" The first thing to do is calculate the *Ŷ* values, and for the simple model"
" that uses only a single predictor we would do the following:"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:37
msgid ""
". create a new column called ``Y.pred`` using the formula ``= 125.97 + "
"(-8.94 * dan.sleep)``"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:40
msgid ""
"Okay, now that we’ve got a variable which stores the regression model "
"predictions for how grumpy I will be on any given day, let’s calculate our "
"sum of squared residuals. We would do that using the following formula:"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:46
msgid ""
". calculate the SS(resid) by creating a new column called ``(Y - Y.pred) ^ "
"2`` using the formula"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:47
msgid "``= (dan.grump - Y.pred) ^ 2``."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:50
msgid ""
". Then, at the bottom of this column calculate the sum of these values,"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:51
msgid "i.e. ``sum((Y - Y.pred) ^ 2)``."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:53
msgid ""
"This should give you a value of **1838.722**. Wonderful. A big number that "
"doesn’t mean very much. Still, let’s forge boldly onwards anyway and "
"calculate the total sum of squares as well. That’s also pretty simple. "
"Calculate the ``SS(tot)`` by:"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:60
msgid ""
". At the bottom of the dan.grump column, calculate the mean value for "
"``dan.grump`` (NB Excel uses the word ``AVERAGE`` rather than ``mean`` in "
"its function)."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:63
msgid ""
". Then create a new column, called ``(Y - mean(Y))^2 )`` using the formula: "
"``= (dan.grump - AVERAGE(dan.grump)) ^ 2`` ."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:66
msgid ""
". Then, at the bottom of this column calculate the sum of these values, i.e."
" ``sum((Y - mean(Y)) ^ 2)``."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:68
msgid ""
"This should give you a value of **9998.59**. Hmm. Well, it’s a much bigger "
"number than the last one, so this does suggest that our regression model was"
" making good predictions. But it’s not very interpretable."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:73
msgid ""
"Perhaps we can fix this. What we’d like to do is to convert these two fairly"
" meaningless numbers into one number. A nice, interpretable number, which "
"for no particular reason we’ll call *R²*. What we would like is for the "
"value of *R²* to be equal to 1 if the regression model makes no errors in "
"predicting the data. In other words, if it turns out that the residual "
"errors are zero. That is, if SS\\ :sub:`res` = 0 then we expect *R²* = 1. "
"Similarly, if the model is completely useless, we would like *R²* to be "
"equal to 0. What do I mean by “useless”? Tempting as it is to demand that "
"the regression model move out of the house, cut its hair and get a real job,"
" I’m probably going to have to pick a more practical definition. In this "
"case, all I mean is that the residual sum of squares is no smaller than the "
"total sum of squares, SS\\ :sub:`res` = SS\\ :sub:`tot`. Wait, why don’t we "
"do exactly that? The formula that provides us with our *R²* value is pretty "
"simple to write down,"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:90
msgid "R² = 1 - (SS\\ :sub:`res` / SS\\ :sub:`tot`)"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:92
msgid "and equally simple to calculate in Excel:"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:94
msgid ". Calculate ``R.squared`` by typing into a blank cell the following:"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:95
msgid "``= 1 - (SS(resid) / SS(tot) )`` ."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:97
msgid ""
"This gives a value for R² of **0.8161018**. The R² value, sometimes called "
"the **coefficient of determination**\\ [#]_ has a simple interpretation: it "
"is the *proportion* of the variance in the outcome variable that can be "
"accounted for by the predictor. So, in this case the fact that we have "
"obtained *R²* = 0.816 means that the predictor (``my.sleep``) explains 81.6%"
" of the variance in the outcome (``my.grump``)."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:104
msgid ""
"Naturally, you don’t actually need to type all these commands into Excel "
"yourself if you want to obtain the *R²* value for your regression model. As "
"we’ll see later on in `Running the hypothesis tests in jamovi "
"<Ch12_Regression_07.html#running-the-hypothesis-tests-in-jamovi>`__, all you"
" need to do is specify this as an option in jamovi. However, let’s put that "
"to one side for the moment. There’s another property of *R²* that I want to "
"point out."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:113
msgid "The relationship between regression and correlation"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:115
msgid ""
"At this point we can revisit my earlier claim that regression, in this very "
"simple form that I’ve discussed so far, is basically the same thing as a "
"correlation. Previously, we used the symbol *r* to denote a Pearson "
"correlation. Might there be some relationship between the value of the "
"correlation coefficient *r* and the *R²* value from linear regression? Of "
"course there is: the squared correlation *r²* is identical to the *R²* value"
" for a linear regression with only a single predictor. In other words, "
"running a Pearson correlation is more or less equivalent to running a linear"
" regression model that uses only one predictor variable."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:126
msgid "The adjusted *R²* (R-squared) value"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:128
msgid ""
"One final thing to point out before moving on. It’s quite common for people "
"to report a slightly different measure of model performance, known as "
"“adjusted *R²*”. The motivation behind calculating the adjusted *R²* value "
"is the observation that adding more predictors into the model will *always* "
"cause the *R²* value to increase (or at least not decrease)."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:135
msgid ""
"The adjusted *R²* value introduces a slight change to the calculation, as "
"follows. For a regression model with K predictors, fit to a data set "
"containing *N* observations, the adjusted *R²* is:"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:140
msgid ""
"\\mbox{adj. } R^2 = 1 - \\left(\\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}} "
"\\times \\frac{N-1}{N-K-1} \\right)"
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:142
msgid ""
"This adjustment is an attempt to take the degrees of freedom into account. "
"The big advantage of the adjusted *R²* value is that when you add more "
"predictors to the model, the adjusted *R²* value will only increase if the "
"new variables improve the model performance more than you’d expect by "
"chance. The big disadvantage is that the adjusted *R²* value *can’t* be "
"interpreted in the elegant way that *R²* can. *R²* has a simple "
"interpretation as the proportion of variance in the outcome variable that is"
" explained by the regression model. To my knowledge, no equivalent "
"interpretation exists for adjusted *R²*."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:153
msgid ""
"An obvious question then is whether you should report *R²* or adjusted *R²*."
" This is probably a matter of personal preference. If you care more about "
"interpretability, then *R²* is better. If you care more about correcting for"
" bias, then adjusted *R²* is probably better. Speaking just for myself, I "
"prefer *R²*. My feeling is that it’s more important to be able to interpret "
"your measure of model performance. Besides, as we’ll see in Section "
"`Hypothesis tests for regression models <Ch12_Regression_07.html#hypothesis-"
"tests-for-regression-models>`__, if you’re worried that the improvement in "
"*R²* that you get by adding a predictor is just due to chance and not "
"because it’s a better model, well we’ve got hypothesis tests for that."
msgstr ""

#: ../../lsj/Ch12_Regression_06.rst:168
msgid ""
"And by “sometimes” I mean “almost never”. In practice everyone just calls it"
" “*R*-squared”."
msgstr ""
