# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The section authors, The jamovi Group, and Sebastian Jentschke (curating this documentation). This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: jamovi\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-08-21 00:07+0200\n"
"PO-Revision-Date: 2020-08-10 17:56+0000\n"
"Language-Team: German (https://www.transifex.com/jamovi/teams/111618/de/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: de\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"

#: ../../lsj/Ch13_ANOVA_02.rst:4
msgid "How ANOVA works"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:6
msgid ""
"In order to answer the question posed by our clinical trial data we’re going"
" to run a one-way ANOVA. I’m going to start by showing you how to do it the "
"hard way, building the statistical tool from the ground up and showing you "
"how you could do it if you didn’t have access to any of the cool built-in "
"ANOVA functions in jamovi. And I hope you’ll read it carefully, try to do it"
" the long way once or twice to make sure you really understand how ANOVA "
"works, and then once you’ve grasped the concept never *ever* do it this way "
"again."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:15
msgid ""
"The experimental design that I described in the previous section strongly "
"suggests that we’re interested in comparing the average mood change for the "
"three different drugs. In that sense, we’re talking about an analysis "
"similar to the *t*-test (Chapter `Comparing two means <Ch11_tTest.html"
"#comparing-two-means>`__) but involving more than two groups. If we let µ\\ "
":sub:`P` denote the population mean for the mood change induced by the "
"placebo, and let µ\\ :sub:`A` and µ\\ :sub:`J` denote the corresponding "
"means for our two drugs, Anxifree and Joyzepam, then the (somewhat "
"pessimistic) null hypothesis that we want to test is that all three "
"population means are identical. That is, *neither* of the two drugs is any "
"more effective than a placebo. We can write out this null hypothesis as:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:28
msgid "H\\ :sub:`0`: it is true that µ\\ :sub:`P` = µ\\ :sub:`A` = µ\\ :sub:`J`"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:30
msgid ""
"As a consequence, our alternative hypothesis is that at least one of the "
"three different treatments is different from the others. It’s a bit tricky "
"to write this mathematically, because (as we’ll discuss) there are quite a "
"few different ways in which the null hypothesis can be false. So for now "
"we’ll just write the alternative hypothesis like this:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:36
msgid "H\\ :sub:`1`: it is NOT true that µ\\ :sub:`P` = µ\\ :sub:`A` = µ\\ :sub:`J`"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:38
msgid ""
"This null hypothesis is a lot trickier to test than any of the ones we’ve "
"seen previously. How shall we do it? A sensible guess would be to “do an "
"ANOVA”, since that’s the title of the chapter, but it’s not particularly "
"clear why an “analysis of *variances*” will help us learn anything useful "
"about the *means*. In fact, this is one of the biggest conceptual "
"difficulties that people have when first encountering ANOVA. To see how this"
" works, I find it most helpful to start by talking about variances. In fact,"
" what I’m going to do is start by playing some mathematical games with the "
"formula that describes the variance. That is, we’ll start out by playing "
"around with variances and it will turn out that this gives us a useful tool "
"for investigating means."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:51
msgid "Two formulas for the variance of *Y*"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:53
msgid ""
"First, let’s start by introducing some notation. We’ll use *G* to refer to "
"the total number of groups. For our data set there are three drugs, so there"
" are *G* = 3 groups. Next, we’ll use *N* to refer to the total sample size; "
"there are a total of *N* = 18 people in our data set. Similarly, let’s use "
"|N_k| to denote the number of people in the *k*-th group. In our fake "
"clinical trial, the sample size is |N_k| = 6` for all three groups.\\ [#]_ "
"Finally, we’ll use *Y* to denote the outcome variable. In our case, *Y* "
"refers to mood change. Specifically, we’ll use |Y_ik| to refer to the mood "
"change experienced by the *i*-th member of the *k*-th group. Similarly, "
"we’ll use |Yb| to be the average mood change, taken across all 18 people in "
"the experiment, and |Yb_k| to refer to the average mood change experienced "
"by the 6 people in group *k*.\\ [#]"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:68
msgid ""
"Now that we’ve got our notation sorted out we can start writing down "
"formulas. To start with, let’s recall the formula for the variance that we "
"used in `Measures of variability <Ch04_Descriptives_2.html#measures-of-"
"variability>`__, way back in those kinder days when we were just doing "
"descriptive statistics. The sample variance of *Y* is defined as follows"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:74
msgid ""
"\\mbox{Var}(Y) = \\frac{1}{N} \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left(Y_{ik} "
"- \\bar{Y} \\right)^2"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:76
msgid ""
"This formula looks pretty much identical to the formula for the variance in "
"`Measures of variability <Ch04_Descriptives_2.html#measures-of-"
"variability>`__. The only difference is that this time around I’ve got two "
"summations here: I’m summing over groups (i.e., values for *k*) and over the"
" people within the groups (i.e., values for *:`i*). This is purely a "
"cosmetic detail. If I’d instead used the notation |Y_p| to refer to the "
"value of the outcome variable for person *p* in the sample, then I’d only "
"have a single summation. The only reason that we have a double summation "
"here is that I’ve classified people into groups, and then assigned numbers "
"to people within groups."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:87
msgid ""
"A concrete example might be useful here. Let’s consider this table, in which"
" we have a total of *N* = 5 people sorted into :*G* = 2 groups. Arbitrarily,"
" let’s say that the “cool” people are group 1 and the “uncool” people are "
"group 2. It turns out that we have three cool people (*N*\\ :sub:`1` = 3) "
"and two uncool people (*N*\\ :sub:`2` = 2)"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:94
msgid "name"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:94
msgid "person"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:94 ../../lsj/Ch13_ANOVA_02.rst:416
#: ../../lsj/Ch13_ANOVA_02.rst:436 ../../lsj/Ch13_ANOVA_02.rst:451
#: ../../lsj/Ch13_ANOVA_02.rst:492 ../../lsj/Ch13_ANOVA_02.rst:515
msgid "group"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:94
msgid "group num."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:94
msgid "index in group"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:94
msgid "grumpiness"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:96
msgid "*p*"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:96 ../../lsj/Ch13_ANOVA_02.rst:417
#: ../../lsj/Ch13_ANOVA_02.rst:437 ../../lsj/Ch13_ANOVA_02.rst:454
#: ../../lsj/Ch13_ANOVA_02.rst:495 ../../lsj/Ch13_ANOVA_02.rst:518
msgid "*k*"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:96
msgid "*i*"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:98
msgid "Ann"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:98 ../../lsj/Ch13_ANOVA_02.rst:98
#: ../../lsj/Ch13_ANOVA_02.rst:98 ../../lsj/Ch13_ANOVA_02.rst:100
#: ../../lsj/Ch13_ANOVA_02.rst:102 ../../lsj/Ch13_ANOVA_02.rst:104
msgid "1"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:98 ../../lsj/Ch13_ANOVA_02.rst:100
#: ../../lsj/Ch13_ANOVA_02.rst:102
msgid "cool"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:98
msgid "20"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:100
msgid "Ben"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:100 ../../lsj/Ch13_ANOVA_02.rst:100
#: ../../lsj/Ch13_ANOVA_02.rst:104 ../../lsj/Ch13_ANOVA_02.rst:106
#: ../../lsj/Ch13_ANOVA_02.rst:106 ../../lsj/Ch13_ANOVA_02.rst:588
msgid "2"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:100
msgid "55"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:102
msgid "Cat"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:102 ../../lsj/Ch13_ANOVA_02.rst:102
msgid "3"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:102
msgid "21"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:104
msgid "Dan"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:104
msgid "4"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:104 ../../lsj/Ch13_ANOVA_02.rst:106
msgid "uncool"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:104
msgid "91"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:106
msgid "Egg"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:106
msgid "5"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:106
msgid "22"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:109
msgid ""
"Notice that I’ve constructed two different labelling schemes here. We have a"
" “person” variable *p* so it would be perfectly sensible to refer to |Y_p| "
"as the grumpiness of the *p*-th person in the sample. For instance, the "
"table shows that Dan is the fourth so we’d say *p* = 4. So, when talking "
"about the grumpiness *Y* of this “Dan” person, whoever he might be, we could"
" refer to his grumpiness by saying that |Y_p| = 91, for person *p* = 4 that "
"is. However, that’s not the only way we could refer to Dan. As an "
"alternative we could note that Dan belongs to the “uncool” group (*k* = 2), "
"and is in fact the first person listed in the uncool group (*i* = 1). So "
"it’s equally valid to refer to Dan’s grumpiness by saying that |Y_ik| = 91, "
"where *k* = 2 and *i* = 1."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:122
msgid ""
"In other words, each person *p* corresponds to a unique *ik* combination, "
"and so the formula that I gave above is actually identical to our original "
"formula for the variance, which would be"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:126
msgid "\\mbox{Var}(Y) = \\frac{1}{N} \\sum_{p=1}^N  \\left(Y_{p} - \\bar{Y} \\right)^2"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:128
msgid ""
"In both formulas, all we’re doing is summing over all of the observations in"
" the sample. Most of the time we would just use the simpler |Y_p| notation; "
"the equation using |Y_p| is clearly the simpler of the two. However, when "
"doing an ANOVA it’s important to keep track of which participants belong in "
"which groups, and we need to use the |Y_ik| notation to do this."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:136
msgid "From variances to sums of squares"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:138
msgid ""
"Okay, now that we’ve got a good grasp on how the variance is calculated, "
"let’s define something called the **total sum of squares**, which is denoted"
" |SS_t|\\. This is very simple. Instead of averaging the squared deviations,"
" which is what we do when calculating the variance, we just add them up."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:144
msgid ""
"So the formula for the total sum of squares is almost identical to the "
"formula for the variance"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:147
msgid ""
"\\mbox{SS}_{tot} = \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left(Y_{ik} - \\bar{Y} "
"\\right)^2"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:149
msgid ""
"When we talk about analysing variances in the context of ANOVA, what we’re "
"really doing is working with the total sums of squares rather than the "
"actual variance. One very nice thing about the total sum of squares is that "
"we can break it up into two different kinds of variation."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:154
msgid ""
"First, we can talk about the **within-group sum of squares**, in which we "
"look to see how different each individual person is from their own group "
"mean"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:158
msgid ""
"\\mbox{SS}_w = \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left( Y_{ik} - \\bar{Y}_k "
"\\right)^2"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:160
msgid ""
"where |Yb_k| is a group mean. In our example, |Yb_k| would be the average "
"mood change experienced by those people given the *k*-th drug. So, instead "
"of comparing individuals to the average of all people in the experiment, "
"we’re only comparing them to those people in the the same group. As a "
"consequence, you’d expect the value of |SS_w| to be smaller than the total "
"sum of squares, because it’s completely ignoring any group differences, "
"i.e., whether the drugs will have different effects on people’s moods."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:169
msgid ""
"Next, we can define a third notion of variation which captures *only* the "
"differences between groups. We do this by looking at the differences between"
" the group means |Yb_k| and grand mean |Yb|."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:174
msgid ""
"In order to quantify the extent of this variation, what we do is calculate "
"the **between-group sum of squares**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:177
msgid ""
"\\begin{aligned} \\mbox{SS}_{b} &=& \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left( "
"\\bar{Y}_k - \\bar{Y} \\right)^2 \\\\               &=& \\sum_{k=1}^G N_k "
"\\left( \\bar{Y}_k - \\bar{Y} \\right)^2\\end{aligned}"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:183
msgid ""
"It’s not too difficult to show that the total variation among people in the "
"experiment |SS_t| is actually the sum of the differences between the groups "
"|SS_b| and the variation inside the groups S\\ :sub:`w`\\. That is,"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:190
msgid "Yay."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:198
msgid ""
"Graphical illustration of “between groups” variation (left panel) and "
"“within groups” variation (right panel). In the left panel, the arrows show "
"the differences in the group means. In the right panel, the arrows highlight"
" the variability within each group."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:205
msgid ""
"Okay, so what have we found out? We’ve discovered that the total variability"
" associated with the outcome variable (|SS_t|\\) can be mathematically "
"carved up into the sum of “the variation due to the differences in the "
"sample means for the different groups” (|SS_b|\\) plus “all the rest of the "
"variation” (|SS_w|\\).\\ [#]_ How does that help me find out whether the "
"groups have different population means? Um. Wait. Hold on a second. Now that"
" I think about it, this is *exactly* what we were looking for. If the null "
"hypothesis is true then you’d expect all the sample means to be pretty "
"similar to each other, right? And that would imply that you’d expect |SS_b| "
"to be really small, or at least you’d expect it to be a lot smaller than "
"“the variation associated with everything else”, |SS_w|\\. Hmm. I detect a "
"hypothesis test coming on."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:221
msgid "From sums of squares to the *F*-test"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:223
msgid ""
"As we saw in the last section, the *qualitative* idea behind ANOVA is to "
"compare the two sums of squares values |SS_b| and |SS_w| to each other. If "
"the between-group variation |SS_b| is large relative to the within-group "
"variation |SS_w| then we have reason to suspect that the population means "
"for the different groups aren’t identical to each other. In order to convert"
" this into a workable hypothesis test, there’s a little bit of “fiddling "
"around” needed. What I’ll do is first show you *what* we do to calculate our"
" test statistic, the ***F* ratio**, and then try to give you a feel for "
"*why* we do it this way."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:234
msgid ""
"In order to convert our SS values into an *F*-ratio the first thing we need "
"to calculate is the **degrees of freedom** associated with the |SS_b| and "
"|SS_w| values. As usual, the degrees of freedom corresponds to the number of"
" unique “data points” that contribute to a particular calculation, minus the"
" number of “constraints” that they need to satisfy. For the within-groups "
"variability what we’re calculating is the variation of the individual "
"observations (*N* data points) around the group means (*G* constraints). In "
"contrast, for the between groups variability we’re interested in the "
"variation of the group means (*G* data points) around the grand mean (1 "
"constraint). Therefore, the degrees of freedom here are:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:247 ../../lsj/Ch13_ANOVA_02.rst:298
msgid "|df_b| = G - 1"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:248 ../../lsj/Ch13_ANOVA_02.rst:302
msgid "|df_w| = N - G"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:250
msgid ""
"Okay, that seems simple enough. What we do next is convert our summed "
"squares value into a “mean squares” value, which we do by dividing by the "
"degrees of freedom:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:254 ../../lsj/Ch13_ANOVA_02.rst:255
#: ../../lsj/Ch13_ANOVA_02.rst:257
msgid ""
"Finally, we calculate the *F*-ratio by dividing the between-groups MS by the"
" within-groups MS:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:260 ../../lsj/Ch13_ANOVA_02.rst:298
msgid "F = |MS_b| / |MS_w|"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:262
msgid ""
"At a very general level, the intuition behind the *F* statistic is "
"straightforward. Bigger values of *F* means that the between-groups "
"variation is large relative to the within-groups variation. As a "
"consequence, the larger the value of *F* the more evidence we have against "
"the null hypothesis. But how large does *F* have to be in order to actually "
"*reject* H\\ :sub:`0`? In order to understand this, you need a slightly "
"deeper understanding of what ANOVA is and what the mean squares values "
"actually are."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:271
msgid ""
"The next section discusses that in a bit of detail, but for readers that "
"aren’t interested in the details of what the test is actually measuring I’ll"
" cut to the chase. In order to complete our hypothesis test we need to know "
"the sampling distribution for *F* if the null hypothesis is true. Not "
"surprisingly, the sampling distribution for the *F* statistic under the null"
" hypothesis is an *F* distribution. If you recall our discussion of the *F* "
"distribution in Chapter `Introduction to probability <Ch07_Probability.html"
"#introduction-to-probability>`__, the *F*-distribution has two parameters, "
"corresponding to the two degrees of freedom involved. The first one *df*\\ "
":sub:`1` is the between groups degrees of freedom |df_b|, and the second one"
" *df*\\ :sub:`2` is the within groups degrees of freedom |df_w|\\."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:284
msgid ""
"A summary of all the key quantities involved in a one-way ANOVA, including "
"the formulas showing how they are calculated, is shown in :numref:`tab-"
"anovatable`."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:289
msgid ""
"All of the key quantities involved in an ANOVA organised into a “standard” "
"ANOVA table. The formulas for all quantities (except the *p*-value which has"
" a very ugly formula and would be nightmarishly hard to calculate without a "
"computer) are shown."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:295 ../../lsj/Ch13_ANOVA_02.rst:585
msgid "df"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:295 ../../lsj/Ch13_ANOVA_02.rst:585
msgid "sum of squares"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:295 ../../lsj/Ch13_ANOVA_02.rst:585
msgid "mean squares"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:295
msgid "*F*- statistic"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:295
msgid "*p*- value"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:298 ../../lsj/Ch13_ANOVA_02.rst:588
msgid "between groups"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:298
msgid "[comp licated]"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:302 ../../lsj/Ch13_ANOVA_02.rst:591
msgid "within groups"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:308
msgid "The model for the data and the meaning of *F*"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:310
msgid ""
"At a fundamental level ANOVA is a competition between two different "
"statistical models, H\\ :sub:`0` and H\\ :sub:`1`. When I described the null"
" and alternative hypotheses at the start of the section, I was a little "
"imprecise about what these models actually are. I’ll remedy that now, though"
" you probably won’t like me for doing so. If you recall, our null hypothesis"
" was that all of the group means are identical to one another. If so, then a"
" natural way to think about the outcome variable |Y_ik| is to describe "
"individual scores in terms of a single population mean *µ*, plus the "
"deviation from that population mean. This deviation is usually denoted ϵ\\ "
":sub:`ik` and is traditionally called the *error* or **residual** associated"
" with that observation. Be careful though. Just like we saw with the word "
"“significant”, the word “error” has a technical meaning in statistics that "
"isn’t quite the same as its everyday English definition. In everyday "
"language, “error” implies a mistake of some kind, but in statistics it "
"doesn’t (or at least, not necessarily). With that in mind, the word "
"“residual” is a better term than the word “error”. In statistics both words "
"mean “leftover variability”, that is “stuff” that the model can’t explain."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:330
msgid ""
"In any case, here’s what the null hypothesis looks like when we write it as "
"a statistical model"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:333
msgid "|Y_ik| = µ + ϵ\\ :sub:`ik`"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:335
msgid ""
"where we make the *assumption* (discussed later) that the residual values "
"ϵ\\ :sub:`ik` are normally distributed, with mean 0 and a standard deviation"
" *σ* that is the same for all groups. To use the notation that we introduced"
" in Chapter `Introduction to probability <Ch07_Probability.html"
"#introduction-to-probability>`__ we would write this assumption like this:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:342
msgid "ϵ\\ :sub:`ik` ~ Normal(0, σ²)"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:344
msgid ""
"What about the alternative hypothesis, H\\ :sub:`1`? The only difference "
"between the null hypothesis and the alternative hypothesis is that we allow "
"each group to have a different population mean. So, if we let µ\\ :sub:`k` "
"denote the population mean for the *k*-th group in our experiment, then the "
"statistical model corresponding to H\\ :sub:`1` is"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:351
msgid "|Y_ik| = µ\\ :sub:`k` + ϵ\\ :sub:`ik`"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:353
msgid ""
"where, once again, we assume that the error terms are normally distributed "
"with mean 0 and standard deviation *σ*. That is, the alternative hypothesis "
"also assumes that ϵ ~ Normal(0, σ²)"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:358
msgid ""
"Okay, now that we’ve described the statistical models underpinning H\\ "
":sub:`0` and H\\ :sub:`1` in more detail, it’s now pretty straightforward to"
" say what the mean square values are measuring, and what this means for the "
"interpretation of *F*. I won’t bore you with the proof of this but it turns "
"out that the within-groups mean square, |MS_w|, can be viewed as an "
"estimator (in the technical sense, Chapter `Estimating unknown quantities "
"from a sample <Ch08_Estimation.html#estimating-unknown-quantities-"
"from-a-sample>`__) of the error variance σ². The between-groups mean square "
"|MS_b| is also an estimator, but what it estimates is the error variance "
"*plus* a quantity that depends on the true differences among the group "
"means. If we call this quantity Q, then we can see that the *F*-statistic is"
" basically:\\ [#]_"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:372
msgid "F = \\frac{\\hat{Q} + \\hat\\sigma^2}{\\hat\\sigma^2}"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:374
msgid ""
"where the true value Q = 0 if the null hypothesis is true, and Q > 0 if the "
"alternative hypothesis is true (`Hays, 1994 <References.html#hays-1994>`__\\"
" , Ch. 10). Therefore, at a bare minimum *the *F* value must be larger than "
"1* to have any chance of rejecting the null hypothesis. Note that this "
"*doesn’t* mean that it’s impossible to get an *F*-value less than 1. What it"
" means is that if the null hypothesis is true the sampling distribution of "
"the *F* ratio has a mean of 1,\\ [#]_ and so we need to see *F*-values "
"larger than 1 in order to safely reject the null."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:384
msgid ""
"To be a bit more precise about the sampling distribution, notice that if the"
" null hypothesis is true, both |MS_b| and |MS_w| are estimators of the "
"variance of the residuals ϵ\\ :sub:`ik`. If those residuals are normally "
"distributed, then you might suspect that the estimate of the variance of ϵ\\"
" :sub:`ik` is χ²-distributed, because (as discussed in `Other useful "
"distributions <Ch07_Probability_6.html#other-useful-distributions>`__) "
"that’s what a χ²-distribution *is*: it’s what you get when you square a "
"bunch of normally-distributed things and add them up. And since the *F* "
"distribution is (again, by definition) what you get when you take the ratio "
"between two things that are χ² distributed, we have our sampling "
"distribution. Obviously, I’m glossing over a whole lot of stuff when I say "
"this, but in broad terms, this really is where our sampling distribution "
"comes from."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:400
msgid "A worked example"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:402
msgid ""
"The previous discussion was fairly abstract and a little on the technical "
"side, so I think that at this point it might be useful to see a worked "
"example. For that, let’s go back to the clinical trial data that I "
"introduced at the start of the chapter. The descriptive statistics that we "
"calculated at the beginning tell us our group means: an average mood gain of"
" 0.45 for the placebo, 0.72 for Anxifree, and 1.48 for Joyzepam. With that "
"in mind, let’s party like it’s 1899\\ [#]_ and start doing some pencil and "
"paper calculations. I’ll only do this for the first 5 observations because "
"it’s not bloody 1899 and I’m very lazy. Let’s start by calculating |SS_w|, "
"the within-group sums of squares. First, let’s draw up a nice table to help "
"us with our calculations:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:416 ../../lsj/Ch13_ANOVA_02.rst:436
#: ../../lsj/Ch13_ANOVA_02.rst:451
msgid "outcome"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:417 ../../lsj/Ch13_ANOVA_02.rst:437
#: ../../lsj/Ch13_ANOVA_02.rst:418 ../../lsj/Ch13_ANOVA_02.rst:419
#: ../../lsj/Ch13_ANOVA_02.rst:420 ../../lsj/Ch13_ANOVA_02.rst:438
#: ../../lsj/Ch13_ANOVA_02.rst:439 ../../lsj/Ch13_ANOVA_02.rst:440
#: ../../lsj/Ch13_ANOVA_02.rst:457 ../../lsj/Ch13_ANOVA_02.rst:459
#: ../../lsj/Ch13_ANOVA_02.rst:461 ../../lsj/Ch13_ANOVA_02.rst:498
#: ../../lsj/Ch13_ANOVA_02.rst:521
msgid "placebo"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:418 ../../lsj/Ch13_ANOVA_02.rst:438
#: ../../lsj/Ch13_ANOVA_02.rst:457
msgid "0.5"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:419 ../../lsj/Ch13_ANOVA_02.rst:439
#: ../../lsj/Ch13_ANOVA_02.rst:459
msgid "0.3"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:420 ../../lsj/Ch13_ANOVA_02.rst:440
#: ../../lsj/Ch13_ANOVA_02.rst:461
msgid "0.1"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:421 ../../lsj/Ch13_ANOVA_02.rst:422
#: ../../lsj/Ch13_ANOVA_02.rst:441 ../../lsj/Ch13_ANOVA_02.rst:442
#: ../../lsj/Ch13_ANOVA_02.rst:463 ../../lsj/Ch13_ANOVA_02.rst:465
#: ../../lsj/Ch13_ANOVA_02.rst:500 ../../lsj/Ch13_ANOVA_02.rst:523
msgid "anxifree"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:421 ../../lsj/Ch13_ANOVA_02.rst:441
#: ../../lsj/Ch13_ANOVA_02.rst:463
msgid "0.6"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:422 ../../lsj/Ch13_ANOVA_02.rst:442
#: ../../lsj/Ch13_ANOVA_02.rst:465
msgid "0.4"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:425
msgid ""
"At this stage, the only thing I’ve included in the table is the raw data "
"itself. That is, the grouping variable (i.e., ``drug``) and outcome variable"
" (i.e. ``mood.gain``) for each person. Note that the outcome variable here "
"corresponds to the |Y_ik| value in our equation previously. The next step in"
" the calculation is to write down, for each person in the study, the "
"corresponding group mean, |Yb_k|. This is slightly repetitive but not "
"particularly difficult since we already calculated those group means when "
"doing our descriptive statistics:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:436
msgid "**group mean**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:437 ../../lsj/Ch13_ANOVA_02.rst:454
#: ../../lsj/Ch13_ANOVA_02.rst:438 ../../lsj/Ch13_ANOVA_02.rst:439
#: ../../lsj/Ch13_ANOVA_02.rst:440
msgid "**0.45**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:441 ../../lsj/Ch13_ANOVA_02.rst:442
msgid "**0.72**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:445
msgid ""
"Now that we’ve written those down, we need to calculate, again for every "
"person, the deviation from the corresponding group mean. That is, we want to"
" subtract |Y_ik| - |Yb_k|. After we’ve done that, we need to square "
"everything. When we do that, here’s what we get:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:451 ../../lsj/Ch13_ANOVA_02.rst:492
msgid "group mean"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:451
msgid "dev. from group mean"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:451
msgid "squared deviation"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:454
msgid "(|Y_ik| - |Yb_k|)"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:454
msgid "(|Y_ik| - |Yb_k|\\)²"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:457 ../../lsj/Ch13_ANOVA_02.rst:459
#: ../../lsj/Ch13_ANOVA_02.rst:461 ../../lsj/Ch13_ANOVA_02.rst:498
msgid "0.45"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:457
msgid "**0.05**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:457
msgid "**0.0025**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:459
msgid "**-0.15**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:459
msgid "**0.0225**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:461
msgid "**-0.35**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:461
msgid "**0.1225**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:463 ../../lsj/Ch13_ANOVA_02.rst:465
#: ../../lsj/Ch13_ANOVA_02.rst:500
msgid "0.72"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:463
msgid "**-0.12**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:463
msgid "**0.0136**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:465
msgid "**-0.32**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:465
msgid "**0.1003**"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:468
msgid ""
"The last step is equally straightforward. In order to calculate the within-"
"group sum of squares we just add up the squared deviations across all "
"observations:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:472
msgid "|SS_w| = 0.0025 + 0.0225 + 0.1225 + 0.0136 + 0.1003 = 0.2614"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:474
msgid ""
"Of course, if we actually wanted to get the *right* answer we’d need to do "
"this for all 18 observations in the data set, not just the first five. We "
"could continue with the pencil and paper calculations if we wanted to, but "
"it’s pretty tedious. Alternatively, it’s not too hard to do this in a "
"dedicated spreadsheet programme such as OpenOffice or Excel. Try and do it "
"yourself. The one that I did, in Excel, is in the file "
"``clinicaltrial_anova.xls``. When you do it you should end up with a within-"
"group sum of squares value of 1.39."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:483
msgid ""
"Okay. Now that we’ve calculated the within groups variation, |SS_w|, it’s "
"time to turn our attention to the between-group sum of squares, |SS_b|. The "
"calculations for this case are very similar. The main difference is that "
"instead of calculating the differences between an observation |Y_ik| and a "
"group mean |Yb_k| for all of the observations, we calculate the differences "
"between the group means |Yb_k| and the grand mean |Yb| (in this case 0.88) "
"for all of the groups."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:492
msgid "grand mean"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:492
msgid "deviation"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:492 ../../lsj/Ch13_ANOVA_02.rst:515
msgid "squared deviations"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:495 ../../lsj/Ch13_ANOVA_02.rst:518
msgid "(|Yb_k| - |Yb|)²"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:498 ../../lsj/Ch13_ANOVA_02.rst:500
#: ../../lsj/Ch13_ANOVA_02.rst:502
msgid "0.88"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:498
msgid "-0.43"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:498 ../../lsj/Ch13_ANOVA_02.rst:521
msgid "0.19"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:500
msgid "-0.16"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:500 ../../lsj/Ch13_ANOVA_02.rst:523
msgid "0.03"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:502 ../../lsj/Ch13_ANOVA_02.rst:525
msgid "joyzepam"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:502
msgid "1.48"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:502
msgid "0.60"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:502 ../../lsj/Ch13_ANOVA_02.rst:525
msgid "0.36"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:505
msgid ""
"However, for the between group calculations we need to multiply each of "
"these squared deviations by |N_k|, the number of observations in the group. "
"We do this because every *observation* in the group (all |N_k| of them) is "
"associated with a between group difference. So if there are six people in "
"the placebo group and the placebo group mean differs from the grand mean by "
"0.19, then the *total* between group variation associated with these six "
"people is 6 · 0.19 = 1.14. So we have to extend our little table of "
"calculations:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:515 ../../lsj/Ch13_ANOVA_02.rst:518
#: ../../lsj/Ch13_ANOVA_02.rst:521 ../../lsj/Ch13_ANOVA_02.rst:523
#: ../../lsj/Ch13_ANOVA_02.rst:525
msgid "..."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:515
msgid "sample size"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:515
msgid "weighted squared dev"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:518
msgid "|N_k| · (|Yb_k| - |Yb|)²"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:521 ../../lsj/Ch13_ANOVA_02.rst:523
#: ../../lsj/Ch13_ANOVA_02.rst:525
msgid "6"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:521
msgid "1.14"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:523
msgid "0.18"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:525
msgid "2.16"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:528
msgid ""
"And so now our between group sum of squares is obtained by summing these "
"“weighted squared deviations” over all three groups in the study:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:531
msgid "|SS_b| = 1.14 + 0.18 + 2.16 = 3.48"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:533
msgid ""
"As you can see, the between group calculations are a lot shorter.\\ [#]_ Now"
" that we’ve calculated our sums of squares values, |SS_b| and |SS_w|, the "
"rest of the ANOVA is pretty painless. The next step is to calculate the "
"degrees of freedom. Since we have *G* = 3 groups and *N* = 18 observations "
"in total our degrees of freedom can be calculated by simple subtraction:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:540
msgid "|df_b| = G - 1 = 2 |df_w| = N - G = 15"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:543
msgid ""
"Next, since we’ve now calculated the values for the sums of squares and the "
"degrees of freedom, for both the within-groups variability and the between-"
"groups variability, we can obtain the mean square values by dividing one by "
"the other:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:548
msgid ""
"\\begin{array}{lclclcl} \\mbox{MS}_b &=& \\displaystyle\\frac{\\mbox{SS}_b "
"}{  \\mbox{df}_b } &=& \\displaystyle\\frac{3.48}{ 2} &=& 1.74 \\\\ "
"\\mbox{MS}_w &=& \\displaystyle\\frac{\\mbox{SS}_w }{  \\mbox{df}_w } &=& "
"\\displaystyle\\frac{1.39}{15} &=& 0.09 \\end{array}"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:555
msgid ""
"We’re almost done. The mean square values can be used to calculate the "
"*F*-value, which is the test statistic that we’re interested in. We do this "
"by dividing the between-groups MS value by the within-groups MS value."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:560
msgid "F = \\frac{\\mbox{MS}_b }{\\mbox{MS}_w} = \\frac{1.74}{0.09} = 19.3"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:562
msgid ""
"Woohooo! This is terribly exciting, yes? Now that we have our test "
"statistic, the last step is to find out whether the test itself gives us a "
"significant result. As discussed in Chapter `Hypothesis testing "
"<Ch09_HypothesisTesting.html#hypothesis-testing>`__ back in the “old days” "
"what we’d do is open up a statistics textbook or flick to the back section "
"which would actually have a huge lookup table and we would find the "
"threshold *F* value corresponding to a particular value of alpha (the null "
"hypothesis rejection region), e.g. 0.05, 0.01 or 0.001, for 2 and 15 degrees"
" of freedom. Doing it this way would give us a threshold *F* value for an "
"alpha of 0.001 of 11.34. As this is less than our calculated *F* value we "
"say that *p* < 0.001. But those were the old days, and nowadays fancy stats "
"software calculates the exact *p*-value for you. In fact, the exact "
"*p*-value is 0.000071. So, unless we’re being *extremely* conservative about"
" our Type I error rate, we’re pretty much guaranteed to reject the null "
"hypothesis."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:579
msgid ""
"At this point, we’re basically done. Having completed our calculations, it’s"
" traditional to organise all these numbers into an ANOVA table like the one "
"in :numref:`tab-anovatable`. For our clinical trial data, the ANOVA table "
"would look like this:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:585
msgid "*F*\\-statistic"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:585
msgid "*p*-value"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:588
msgid "3.48"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:588
msgid "1.74"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:588
msgid "19.3"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:588
msgid "0.000071"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:591
msgid "15"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:591
msgid "1.39"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:591
msgid "0.09"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:595
msgid ""
"These days, you’ll probably never have much reason to want to construct one "
"of these tables yourself, but you will find that almost all statistical "
"software (jamovi included) tends to organise the output of an ANOVA into a "
"table like this, so it’s a good idea to get used to reading them. However, "
"although the software will output a full ANOVA table, there’s almost never a"
" good reason to include the whole table in your write up. A pretty standard "
"way of reporting this result would be to write something like this:"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:604
msgid ""
"One-way ANOVA showed a significant effect of drug on mood gain: *F*\\(2,15) "
"= 19.3, *p* < 0.001."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:607
msgid "Sigh. So much work for one short sentence."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:612
msgid ""
"When all groups have the same number of observations, the experimental "
"design is said to be “balanced”. Balance isn’t such a big deal for one-way "
"ANOVA, which is the topic of this chapter. It becomes more important when "
"you start doing more complicated ANOVAs."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:618
msgid ""
"|SS_w| is also referred to in an independent ANOVA as the error variance, or"
" SS\\ :sub:`error`"
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:622
msgid ""
"If you read ahead to Chapter `Factorial ANOVA <Ch14_ANOVA2.html#factorial-"
"anova>`__ and look at how the “treatment effect” at level k of a factor is "
"defined in terms of the α\\ :sub:`k` values (see Section `Factorial ANOVA 2:"
" balanced designs, interactions allowed <Ch14_ANOVA2_02.html#factorial-"
"anova-2-balanced-designs-interactions-allowed>`__), it turns out that Q "
"refers to a weighted mean of the squared treatment effects, :math:`Q = "
"(\\sum_{k=1}^G N_k \\alpha_k^2)/(G-1)`."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:631
msgid ""
"Or, if we want to be sticklers for accuracy, :math:`1 + \\frac{2}{df_2 - "
"2}`."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:635
msgid ""
"Or, to be precise, party like “it’s 1899 and we’ve got no friends and "
"nothing better to do with our time than do some calculations that wouldn’t "
"have made any sense in 1899 because ANOVA didn’t exist until about the "
"1920s”."
msgstr ""

#: ../../lsj/Ch13_ANOVA_02.rst:641
msgid ""
"In the Excel ``clinicaltrial_anova.xls`` the value for |SS_b| worked out to "
"be very slightly different, 3.45, than that shown in the text above "
"(rounding errors!)"
msgstr ""
