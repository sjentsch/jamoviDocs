# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The section authors, The jamovi Group, and Sebastian Jentschke (curating this documentation). This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: jamovi\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-08-18 23:13+0200\n"
"PO-Revision-Date: 2020-08-10 17:57+0000\n"
"Language-Team: German (https://www.transifex.com/jamovi/teams/111618/de/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: de\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"

#: ../../lsj/Ch15_FactorAnalysis_2.rst:4
msgid "Principal Component Analysis"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:6
msgid ""
"In the previous section we saw that EFA works to identify underlying latent "
"factors. And, as we saw, in one scenario the smaller number of latent "
"factors can be used in further statistical analysis using some sort of "
"combined factor scores."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:11
msgid ""
"In this way EFA is being used as a “data reduction” technique. Another type "
"of data reduction technique, sometimes seen as part of the EFA family, is "
"**principal component analysis (PCA)**. However, PCA does not identify "
"underlying latent factors. Instead it creates a linear composite score from "
"a larger set of measured variables."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:17
msgid ""
"PCA simply produces a mathematical transformation to the original data with "
"no assumptions about how the variables co-vary. The aim of PCA is to "
"calculate a few linear combinations (components) of the original variables "
"that can be used to summarize the observed data set without losing much "
"information. However, if identification of underlying structure is a goal of"
" the analysis, then EFA is to be preferred. And, as we saw, EFA produces "
"factor scores that can be used for data reduction purposes just like "
"principal component scores (`Fabrigar et al., 1999 "
"<References.html#fabrigar-1999>`__\\ )."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:26
msgid ""
"PCA has been popular in Psychology for a number of reasons, and therefore "
"it’s worth covering, although nowadays EFA is just as easy to do given the "
"power of desktop computers and can be less susceptible to bias than PCA, "
"especially with a small number of factors and variables. We’ll use the same "
"``bfi_sample`` dataset as before. Much of the procedure is similar to EFA, "
"so although there are some conceptual differences, practically the steps are"
" the same\\ [#]_, and with large samples and a sufficient number of factors "
"and variables, the results from PCA and EFA should be fairly similar."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:36
msgid "Performing PCA in jamovi"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:38
msgid ""
"Once you have loaded up the ``bfi_sample`` dataset, select ``Factor`` → "
"``Principal Component Analysis`` from the ``Analyses`` ribbbon menu to open "
"the analysis panel where you can determine the settings for the PCA (:numref"
":`fig-pca1`). Then select the 25 personality questions and transfer them "
"into the ‘Variables’ box. Check appropriate options, including ``Assumption "
"Checks``, but also ``Rotation`` under ``Method``, ``Number of Factors`` to "
"extract, and the ``Additional Output`` options (see :numref:`fig-pca1` for "
"suggested options for this PCA, and please note that ``Rotation`` under "
"``Method`` and ``Number of Factors`` extracted is typically adjusted during "
"the analysis to find the best result, as described below)."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:55
msgid ""
"Analysis panel with the settings for conducting a Principal Component "
"Analysis (PCA) in jamovi"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:60
msgid ""
"First, checking the assumptions (see :numref:`fig-pca2`), you can see that "
"(1) Bartlett’s test of sphericity is significant, so this assumption is "
"satisfied; and (2) the KMO measure of sampling adequacy (MSA) is 0.81 "
"overall, suggesting very good sampling adequacy. No problems here then!"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:71
msgid ""
"Assumption checks for the personality item data when using a PCA in jamovi"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:75
msgid ""
"The next thing to check is how many components to use (or “extract” from the"
" data). As with EFA, three different approaches are available:"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:78
msgid ""
"One convention is to choose all components with Eigen values greater than 1."
" This would give us two components with our data."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:81
msgid ""
"Examination of the scree plot, as in :numref:`fig-pca3`, lets you identify "
"the “point of inflection”. This is the point at which the slope of the scree"
" curve clearly levels off, below the “elbow”. Again, this would give us two "
"components as the levelling off clearly occurs after the second component."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:87
msgid ""
"Using a parallel analysis technique, the obtained Eigen values are compared "
"to those that would be obtained from random data. The number of components "
"extracted is the number with Eigen values greater than what would be found "
"with random data."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:98
msgid ""
"Scree plot of the personality item data when conducting a PCA in jamovi, "
"showing the levelling off point, the “elbow”, after component 5"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:103
msgid ""
"The third approach is a good one according to `Fabrigar et al. (1999) "
"<References.html#fabrigar-1999>`__, although in practice researchers tend to"
" look at all three and then make a judgement about the number of components "
"that are most easily or helpfully interpreted. This can be understood as the"
" “meaningfulness criterion”, and researchers will typically examine, in "
"addition to the solution from one of the approaches above, solutions with "
"one or two more or fewer components. They then adopt the solution which "
"makes the most sense to them."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:112
msgid ""
"At the same time, we should also consider the best way to rotate the final "
"solution. Again, as with EFA, there are two main approaches to rotation: "
"orthogonal (e.g. ``Varimax``) rotation forces the selected components to be "
"uncorrelated; whereas oblique (e.g. ``Oblimin``) rotation allows the "
"selected components to be correlated. Dimensions of interest to "
"psychologists and behavioural scientists are not often dimensions we would "
"expect to be orthogonal, so oblique solutions are arguably more sensible. "
"Practically, if in an oblique rotation the components are found to be "
"substantially correlated (i.e. > 0.3) then this would confirm our intuition "
"to prefer oblique rotation. If the components are, in fact, correlated, then"
" an oblique rotation will produce a better estimate of the true components "
"and a better simple structure than will an orthogonal rotation. And, if the "
"oblique rotation indicates that the components have close to zero "
"correlations between one another, then the researcher can go ahead and "
"conduct an orthogonal rotation (which should then give about the same "
"solution as the oblique rotation). In :numref:`fig-pca4` we see that none of"
" the correlations is > 0.3 so it is appropriate to switch to orthogonal "
"(``Varimax``) rotation."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:136
msgid ""
"Component summary statistics and correlations for a five component solution "
"when conducting a PCA with the personality item data in jamovi"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:141
msgid ""
"In :numref:`fig-pca4` we also have the proportion of overall variance in the"
" data that is accounted for by the two components. Components one and two "
"account for just over 12% of the variance each. Taken together, the five "
"component solution accounts for just over half of the variance (56%) in the "
"observed data. Be aware that in every PCA you could potentially have the "
"same number of components as observed variables, but every additional "
"component you include will add a smaller amount of explained variance. If "
"the first few components explain a good amount of the variance in the "
"original 25 variables, then those components are clearly a useful, simpler "
"substitute for all 25 variables. You can drop the rest without losing too "
"much of the original variability. But if it takes 18 components to explain "
"most of the variance in those 25 variables, you might as well just use the "
"original 25."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:154
msgid ""
":numref:`fig-pca5` shows the component loadings. That’s is, how the 25 "
"different personality items load onto each of the selected components. We "
"have hidden loadings less than 0.4 (set in the options shown in :numref"
":`fig-pca1`) as we were interested in items with a substantive loading and "
"setting the threshold at the higher 0.4 value also provided a cleaner, "
"clearer solution."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:166
msgid ""
"Component loadings for a five component solution when conducting a PCA with "
"the personality item data in jamovi"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:171
msgid ""
"For components 1, 2, 3 and 4 the pattern of component loadings closely "
"matches the putative factors specified in :numref:`fig-efa2`. And component "
"5 is pretty close, with four of the five observed variables that putatively "
"measure “Openness” loading pretty well onto the component. Variable ``O4`` "
"doesn’t quite seem to fit though, as the component solution in :numref:`fig-"
"pca5` suggests that it loads onto component 4 (albeit with a relatively low "
"loading) but not substantively onto component 5."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:179
msgid ""
"We can also see in :numref:`fig-pca1` the “uniqueness” of each variable. "
"Uniqueness is the proportion of variance that is ‘unique’ to the variable "
"and not explained by the components. For example, 58% of the variance in "
"``A1`` is not explained by the components in the five component solution. In"
" contrast, ``N1`` has relatively low variance not accounted for by the "
"component solution (30%). Note that the greater the ‘uniqueness’, the lower "
"the relevance or contribution of the variable in the component model."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:187
msgid ""
"Hopefully, this has given you a good first idea about how to undertake PCA "
"in jamovi, and how it is conceptually different but practically fairly "
"similar (given the right data) to EFA."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:191
msgid ""
"You can go on to create component scores in much the same way as in EFA. "
"However, if you take the option to create an optimally-weighted component "
"score index then the commands and syntax in the jamovi ``Rj`` editor are a "
"little different. See :numref:`fig-pca6`."
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:202
msgid ""
"Rj editor commands for creating optimally weighted component scores for the "
"five component solution  when conducting a PCA with the personality item "
"data in jamovi"
msgstr ""

#: ../../lsj/Ch15_FactorAnalysis_2.rst:211
msgid ""
"...and that means there is a fair bit of repetition in the PCA steps set out"
" in the next section. Sorry about that, but hopefully it is not too bad!"
msgstr ""
